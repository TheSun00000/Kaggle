{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-08-25T21:49:18.847479Z","iopub.execute_input":"2021-08-25T21:49:18.847837Z","iopub.status.idle":"2021-08-25T21:49:18.868128Z","shell.execute_reply.started":"2021-08-25T21:49:18.847809Z","shell.execute_reply":"2021-08-25T21:49:18.867134Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"/kaggle/input/blending/train_oof.csv\n/kaggle/input/blending/__results__.html\n/kaggle/input/blending/__notebook_source__.ipynb\n/kaggle/input/blending/__notebook__.ipynb\n/kaggle/input/blending/__output__.json\n/kaggle/input/blending/oof.csv\n/kaggle/input/blending/test_oof.csv\n/kaggle/input/blending/custom.css\n/kaggle/input/tabular-playground-series-aug-2021/sample_submission.csv\n/kaggle/input/tabular-playground-series-aug-2021/train.csv\n/kaggle/input/tabular-playground-series-aug-2021/test.csv\n","output_type":"stream"}]},{"cell_type":"code","source":"import xgboost as xgb\nimport lightgbm as lgb\nimport catboost as ctb\nfrom sklearn.model_selection import KFold, train_test_split\nfrom sklearn.linear_model import LinearRegression, ElasticNet, BayesianRidge, Lasso\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.metrics import mean_squared_error\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns","metadata":{"execution":{"iopub.status.busy":"2021-08-25T21:49:19.763590Z","iopub.execute_input":"2021-08-25T21:49:19.763921Z","iopub.status.idle":"2021-08-25T21:49:22.381986Z","shell.execute_reply.started":"2021-08-25T21:49:19.763890Z","shell.execute_reply":"2021-08-25T21:49:22.380937Z"},"trusted":true},"execution_count":3,"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<style type='text/css'>\n.datatable table.frame { margin-bottom: 0; }\n.datatable table.frame thead { border-bottom: none; }\n.datatable table.frame tr.coltypes td {  color: #FFFFFF;  line-height: 6px;  padding: 0 0.5em;}\n.datatable .bool    { background: #DDDD99; }\n.datatable .object  { background: #565656; }\n.datatable .int     { background: #5D9E5D; }\n.datatable .float   { background: #4040CC; }\n.datatable .str     { background: #CC4040; }\n.datatable .time    { background: #40CC40; }\n.datatable .row_index {  background: var(--jp-border-color3);  border-right: 1px solid var(--jp-border-color0);  color: var(--jp-ui-font-color3);  font-size: 9px;}\n.datatable .frame tbody td { text-align: left; }\n.datatable .frame tr.coltypes .row_index {  background: var(--jp-border-color0);}\n.datatable th:nth-child(2) { padding-left: 12px; }\n.datatable .hellipsis {  color: var(--jp-cell-editor-border-color);}\n.datatable .vellipsis {  background: var(--jp-layout-color0);  color: var(--jp-cell-editor-border-color);}\n.datatable .na {  color: var(--jp-cell-editor-border-color);  font-size: 80%;}\n.datatable .sp {  opacity: 0.25;}\n.datatable .footer { font-size: 9px; }\n.datatable .frame_dimensions {  background: var(--jp-border-color3);  border-top: 1px solid var(--jp-border-color0);  color: var(--jp-ui-font-color3);  display: inline-block;  opacity: 0.6;  padding: 1px 10px 1px 5px;}\n</style>\n"},"metadata":{}}]},{"cell_type":"code","source":"train_oof = pd.read_csv(\"/kaggle/input/blending/train_oof.csv\", index_col=\"id\")\ntest_oof  = pd.read_csv(\"/kaggle/input/blending/test_oof.csv\", index_col=\"id\")\n\ntrain = pd.read_csv(\"/kaggle/input/tabular-playground-series-aug-2021/train.csv\", index_col=\"id\")\ntarget = train[\"loss\"]\ntrain = train.drop(\"loss\", axis=1)\ntest = pd.read_csv(\"/kaggle/input/tabular-playground-series-aug-2021/test.csv\", index_col=\"id\")\n\ntrain_oof = train_oof.drop([\"lr\", \"elst\", \"br\"], axis=1)\ntest_oof = test_oof.drop([\"lr\", \"elst\", \"br\"], axis=1)","metadata":{"execution":{"iopub.status.busy":"2021-08-25T21:49:22.383734Z","iopub.execute_input":"2021-08-25T21:49:22.384127Z","iopub.status.idle":"2021-08-25T21:49:34.172593Z","shell.execute_reply.started":"2021-08-25T21:49:22.384086Z","shell.execute_reply":"2021-08-25T21:49:34.171737Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"train = pd.concat([train, train_oof],axis=1)\ntest  = pd.concat([test , test_oof ],axis=1)","metadata":{"execution":{"iopub.status.busy":"2021-08-25T21:49:34.174086Z","iopub.execute_input":"2021-08-25T21:49:34.174357Z","iopub.status.idle":"2021-08-25T21:49:34.299943Z","shell.execute_reply.started":"2021-08-25T21:49:34.174332Z","shell.execute_reply":"2021-08-25T21:49:34.298964Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"import optuna \nimport optuna.integration.lightgbm as lgbo\n\nparams = {'objective': 'poisson',  'metric': 'rmse' } #'objective': 'mean_squared_error',\nfrom sklearn.metrics import mean_absolute_error\nx_train, x_test, y_train, y_test = train_test_split(train, target, test_size=0.3, random_state=42)\nlgb_train = lgb.Dataset(x_train, y_train)\nlgb_valid = lgb.Dataset(x_test, y_test)\nmodel = lgbo.train(params, lgb_train, valid_sets=[lgb_valid], verbose_eval=False, num_boost_round=100, early_stopping_rounds=5) \nmodel.params","metadata":{"execution":{"iopub.status.busy":"2021-08-25T21:51:26.222595Z","iopub.execute_input":"2021-08-25T21:51:26.222953Z","iopub.status.idle":"2021-08-25T21:56:35.602107Z","shell.execute_reply.started":"2021-08-25T21:51:26.222922Z","shell.execute_reply":"2021-08-25T21:56:35.601225Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":6,"outputs":[{"name":"stderr","text":"\u001b[32m[I 2021-08-25 21:51:27,181]\u001b[0m A new study created in memory with name: no-name-b81eaf16-ecd6-49a1-ade2-b6bb8bbfd418\u001b[0m\nfeature_fraction, val_score: inf:   0%|          | 0/7 [00:00<?, ?it/s]","output_type":"stream"},{"name":"stdout","text":"[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.012522 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 26263\n[LightGBM] [Info] Number of data points in the train set: 175000, number of used features: 103\n[LightGBM] [Info] Start training from score 1.920970\n","output_type":"stream"},{"name":"stderr","text":"feature_fraction, val_score: 7.845767:  14%|#4        | 1/7 [00:04<00:25,  4.33s/it]\u001b[32m[I 2021-08-25 21:51:31,522]\u001b[0m Trial 0 finished with value: 7.845767225662967 and parameters: {'feature_fraction': 0.4}. Best is trial 0 with value: 7.845767225662967.\u001b[0m\nfeature_fraction, val_score: 7.845767:  14%|#4        | 1/7 [00:04<00:25,  4.33s/it]","output_type":"stream"},{"name":"stdout","text":"[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.116877 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 26263\n[LightGBM] [Info] Number of data points in the train set: 175000, number of used features: 103\n[LightGBM] [Info] Start training from score 1.920970\n","output_type":"stream"},{"name":"stderr","text":"feature_fraction, val_score: 7.844309:  29%|##8       | 2/7 [00:10<00:26,  5.28s/it]\u001b[32m[I 2021-08-25 21:51:37,469]\u001b[0m Trial 1 finished with value: 7.844309377275512 and parameters: {'feature_fraction': 0.8999999999999999}. Best is trial 1 with value: 7.844309377275512.\u001b[0m\nfeature_fraction, val_score: 7.844309:  29%|##8       | 2/7 [00:10<00:26,  5.28s/it]","output_type":"stream"},{"name":"stdout","text":"[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.117406 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 26263\n[LightGBM] [Info] Number of data points in the train set: 175000, number of used features: 103\n[LightGBM] [Info] Start training from score 1.920970\n","output_type":"stream"},{"name":"stderr","text":"feature_fraction, val_score: 7.844309:  43%|####2     | 3/7 [00:14<00:19,  4.86s/it]\u001b[32m[I 2021-08-25 21:51:41,823]\u001b[0m Trial 2 finished with value: 7.84588209928003 and parameters: {'feature_fraction': 0.7}. Best is trial 1 with value: 7.844309377275512.\u001b[0m\nfeature_fraction, val_score: 7.844309:  43%|####2     | 3/7 [00:14<00:19,  4.86s/it]","output_type":"stream"},{"name":"stdout","text":"[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.102344 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 26263\n[LightGBM] [Info] Number of data points in the train set: 175000, number of used features: 103\n[LightGBM] [Info] Start training from score 1.920970\n","output_type":"stream"},{"name":"stderr","text":"feature_fraction, val_score: 7.844309:  57%|#####7    | 4/7 [00:18<00:13,  4.58s/it]\u001b[32m[I 2021-08-25 21:51:45,980]\u001b[0m Trial 3 finished with value: 7.844673768054259 and parameters: {'feature_fraction': 0.5}. Best is trial 1 with value: 7.844309377275512.\u001b[0m\nfeature_fraction, val_score: 7.844309:  57%|#####7    | 4/7 [00:18<00:13,  4.58s/it]","output_type":"stream"},{"name":"stdout","text":"[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.109644 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 26263\n[LightGBM] [Info] Number of data points in the train set: 175000, number of used features: 103\n[LightGBM] [Info] Start training from score 1.920970\n","output_type":"stream"},{"name":"stderr","text":"feature_fraction, val_score: 7.844309:  71%|#######1  | 5/7 [00:22<00:08,  4.43s/it]\u001b[32m[I 2021-08-25 21:51:50,141]\u001b[0m Trial 4 finished with value: 7.845987984360467 and parameters: {'feature_fraction': 0.6}. Best is trial 1 with value: 7.844309377275512.\u001b[0m\nfeature_fraction, val_score: 7.844309:  71%|#######1  | 5/7 [00:22<00:08,  4.43s/it]","output_type":"stream"},{"name":"stdout","text":"[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.122879 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 26263\n[LightGBM] [Info] Number of data points in the train set: 175000, number of used features: 103\n[LightGBM] [Info] Start training from score 1.920970\n","output_type":"stream"},{"name":"stderr","text":"feature_fraction, val_score: 7.844260:  86%|########5 | 6/7 [00:27<00:04,  4.62s/it]\u001b[32m[I 2021-08-25 21:51:55,121]\u001b[0m Trial 5 finished with value: 7.844260247260994 and parameters: {'feature_fraction': 1.0}. Best is trial 5 with value: 7.844260247260994.\u001b[0m\nfeature_fraction, val_score: 7.844260:  86%|########5 | 6/7 [00:27<00:04,  4.62s/it]","output_type":"stream"},{"name":"stdout","text":"[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.118193 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 26263\n[LightGBM] [Info] Number of data points in the train set: 175000, number of used features: 103\n[LightGBM] [Info] Start training from score 1.920970\n","output_type":"stream"},{"name":"stderr","text":"feature_fraction, val_score: 7.844260: 100%|##########| 7/7 [00:32<00:00,  4.55s/it]\u001b[32m[I 2021-08-25 21:51:59,541]\u001b[0m Trial 6 finished with value: 7.845276501398708 and parameters: {'feature_fraction': 0.8}. Best is trial 5 with value: 7.844260247260994.\u001b[0m\nfeature_fraction, val_score: 7.844260: 100%|##########| 7/7 [00:32<00:00,  4.62s/it]\nnum_leaves, val_score: 7.844260:   0%|          | 0/20 [00:00<?, ?it/s]","output_type":"stream"},{"name":"stdout","text":"[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.115635 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 26263\n[LightGBM] [Info] Number of data points in the train set: 175000, number of used features: 103\n[LightGBM] [Info] Start training from score 1.920970\n","output_type":"stream"},{"name":"stderr","text":"num_leaves, val_score: 7.844260:   5%|5         | 1/20 [00:06<02:08,  6.74s/it]\u001b[32m[I 2021-08-25 21:52:06,285]\u001b[0m Trial 7 finished with value: 7.853543983169682 and parameters: {'num_leaves': 217}. Best is trial 7 with value: 7.853543983169682.\u001b[0m\nnum_leaves, val_score: 7.844260:   5%|5         | 1/20 [00:06<02:08,  6.74s/it]","output_type":"stream"},{"name":"stdout","text":"[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.132562 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 26263\n[LightGBM] [Info] Number of data points in the train set: 175000, number of used features: 103\n[LightGBM] [Info] Start training from score 1.920970\n","output_type":"stream"},{"name":"stderr","text":"num_leaves, val_score: 7.844260:  10%|#         | 2/20 [00:12<01:52,  6.23s/it]\u001b[32m[I 2021-08-25 21:52:12,159]\u001b[0m Trial 8 finished with value: 7.845941923144947 and parameters: {'num_leaves': 72}. Best is trial 8 with value: 7.845941923144947.\u001b[0m\nnum_leaves, val_score: 7.844260:  10%|#         | 2/20 [00:12<01:52,  6.23s/it]","output_type":"stream"},{"name":"stdout","text":"[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.122984 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 26263\n[LightGBM] [Info] Number of data points in the train set: 175000, number of used features: 103\n[LightGBM] [Info] Start training from score 1.920970\n","output_type":"stream"},{"name":"stderr","text":"num_leaves, val_score: 7.844260:  15%|#5        | 3/20 [00:19<01:49,  6.47s/it]\u001b[32m[I 2021-08-25 21:52:18,909]\u001b[0m Trial 9 finished with value: 7.851428822098783 and parameters: {'num_leaves': 190}. Best is trial 8 with value: 7.845941923144947.\u001b[0m\nnum_leaves, val_score: 7.844260:  15%|#5        | 3/20 [00:19<01:49,  6.47s/it]","output_type":"stream"},{"name":"stdout","text":"[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.124268 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 26263\n[LightGBM] [Info] Number of data points in the train set: 175000, number of used features: 103\n[LightGBM] [Info] Start training from score 1.920970\n","output_type":"stream"},{"name":"stderr","text":"num_leaves, val_score: 7.844260:  20%|##        | 4/20 [00:25<01:43,  6.48s/it]\u001b[32m[I 2021-08-25 21:52:25,409]\u001b[0m Trial 10 finished with value: 7.846375587698662 and parameters: {'num_leaves': 118}. Best is trial 8 with value: 7.845941923144947.\u001b[0m\nnum_leaves, val_score: 7.844260:  20%|##        | 4/20 [00:25<01:43,  6.48s/it]","output_type":"stream"},{"name":"stdout","text":"[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.119349 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 26263\n[LightGBM] [Info] Number of data points in the train set: 175000, number of used features: 103\n[LightGBM] [Info] Start training from score 1.920970\n","output_type":"stream"},{"name":"stderr","text":"num_leaves, val_score: 7.844260:  25%|##5       | 5/20 [00:31<01:32,  6.19s/it]\u001b[32m[I 2021-08-25 21:52:31,083]\u001b[0m Trial 11 finished with value: 7.845700357670329 and parameters: {'num_leaves': 78}. Best is trial 11 with value: 7.845700357670329.\u001b[0m\nnum_leaves, val_score: 7.844260:  25%|##5       | 5/20 [00:31<01:32,  6.19s/it]","output_type":"stream"},{"name":"stdout","text":"[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.121522 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 26263\n[LightGBM] [Info] Number of data points in the train set: 175000, number of used features: 103\n[LightGBM] [Info] Start training from score 1.920970\n","output_type":"stream"},{"name":"stderr","text":"num_leaves, val_score: 7.844260:  30%|###       | 6/20 [00:36<01:21,  5.80s/it]\u001b[32m[I 2021-08-25 21:52:36,120]\u001b[0m Trial 12 finished with value: 7.84529580432695 and parameters: {'num_leaves': 30}. Best is trial 12 with value: 7.84529580432695.\u001b[0m\nnum_leaves, val_score: 7.844260:  30%|###       | 6/20 [00:36<01:21,  5.80s/it]","output_type":"stream"},{"name":"stdout","text":"[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.122105 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 26263\n[LightGBM] [Info] Number of data points in the train set: 175000, number of used features: 103\n[LightGBM] [Info] Start training from score 1.920970\n","output_type":"stream"},{"name":"stderr","text":"num_leaves, val_score: 7.844260:  35%|###5      | 7/20 [00:42<01:17,  5.98s/it]\u001b[32m[I 2021-08-25 21:52:42,472]\u001b[0m Trial 13 finished with value: 7.844791779182203 and parameters: {'num_leaves': 43}. Best is trial 13 with value: 7.844791779182203.\u001b[0m\nnum_leaves, val_score: 7.844260:  35%|###5      | 7/20 [00:42<01:17,  5.98s/it]","output_type":"stream"},{"name":"stdout","text":"[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.120855 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 26263\n[LightGBM] [Info] Number of data points in the train set: 175000, number of used features: 103\n[LightGBM] [Info] Start training from score 1.920970\n","output_type":"stream"},{"name":"stderr","text":"num_leaves, val_score: 7.844260:  40%|####      | 8/20 [00:49<01:12,  6.06s/it]\u001b[32m[I 2021-08-25 21:52:48,718]\u001b[0m Trial 14 finished with value: 7.849008776863846 and parameters: {'num_leaves': 128}. Best is trial 13 with value: 7.844791779182203.\u001b[0m\nnum_leaves, val_score: 7.844260:  40%|####      | 8/20 [00:49<01:12,  6.06s/it]","output_type":"stream"},{"name":"stdout","text":"[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.117368 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 26263\n[LightGBM] [Info] Number of data points in the train set: 175000, number of used features: 103\n[LightGBM] [Info] Start training from score 1.920970\n","output_type":"stream"},{"name":"stderr","text":"num_leaves, val_score: 7.844260:  45%|####5     | 9/20 [00:56<01:10,  6.44s/it]\u001b[32m[I 2021-08-25 21:52:55,980]\u001b[0m Trial 15 finished with value: 7.853332378348966 and parameters: {'num_leaves': 228}. Best is trial 13 with value: 7.844791779182203.\u001b[0m\nnum_leaves, val_score: 7.844260:  45%|####5     | 9/20 [00:56<01:10,  6.44s/it]","output_type":"stream"},{"name":"stdout","text":"[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.126396 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 26263\n[LightGBM] [Info] Number of data points in the train set: 175000, number of used features: 103\n[LightGBM] [Info] Start training from score 1.920970\n","output_type":"stream"},{"name":"stderr","text":"num_leaves, val_score: 7.844260:  50%|#####     | 10/20 [01:01<01:01,  6.11s/it]\u001b[32m[I 2021-08-25 21:53:01,365]\u001b[0m Trial 16 finished with value: 7.8467392330806796 and parameters: {'num_leaves': 84}. Best is trial 13 with value: 7.844791779182203.\u001b[0m\nnum_leaves, val_score: 7.844260:  50%|#####     | 10/20 [01:01<01:01,  6.11s/it]","output_type":"stream"},{"name":"stdout","text":"[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.125336 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 26263\n[LightGBM] [Info] Number of data points in the train set: 175000, number of used features: 103\n[LightGBM] [Info] Start training from score 1.920970\n","output_type":"stream"},{"name":"stderr","text":"num_leaves, val_score: 7.844260:  55%|#####5    | 11/20 [01:05<00:49,  5.48s/it]\u001b[32m[I 2021-08-25 21:53:05,404]\u001b[0m Trial 17 finished with value: 7.845534438387922 and parameters: {'num_leaves': 2}. Best is trial 13 with value: 7.844791779182203.\u001b[0m\nnum_leaves, val_score: 7.844260:  55%|#####5    | 11/20 [01:05<00:49,  5.48s/it]","output_type":"stream"},{"name":"stdout","text":"[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.120921 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 26263\n[LightGBM] [Info] Number of data points in the train set: 175000, number of used features: 103\n[LightGBM] [Info] Start training from score 1.920970\n","output_type":"stream"},{"name":"stderr","text":"num_leaves, val_score: 7.843588:  60%|######    | 12/20 [01:11<00:43,  5.44s/it]\u001b[32m[I 2021-08-25 21:53:10,766]\u001b[0m Trial 18 finished with value: 7.843588209976873 and parameters: {'num_leaves': 10}. Best is trial 18 with value: 7.843588209976873.\u001b[0m\nnum_leaves, val_score: 7.843588:  60%|######    | 12/20 [01:11<00:43,  5.44s/it]","output_type":"stream"},{"name":"stdout","text":"[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.121729 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 26263\n[LightGBM] [Info] Number of data points in the train set: 175000, number of used features: 103\n[LightGBM] [Info] Start training from score 1.920970\n","output_type":"stream"},{"name":"stderr","text":"num_leaves, val_score: 7.843588:  65%|######5   | 13/20 [01:17<00:39,  5.59s/it]\u001b[32m[I 2021-08-25 21:53:16,678]\u001b[0m Trial 19 finished with value: 7.84475015455396 and parameters: {'num_leaves': 22}. Best is trial 18 with value: 7.843588209976873.\u001b[0m\nnum_leaves, val_score: 7.843588:  65%|######5   | 13/20 [01:17<00:39,  5.59s/it]","output_type":"stream"},{"name":"stdout","text":"[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.118975 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 26263\n[LightGBM] [Info] Number of data points in the train set: 175000, number of used features: 103\n[LightGBM] [Info] Start training from score 1.920970\n","output_type":"stream"},{"name":"stderr","text":"num_leaves, val_score: 7.843588:  70%|#######   | 14/20 [01:22<00:32,  5.43s/it]\u001b[32m[I 2021-08-25 21:53:21,763]\u001b[0m Trial 20 finished with value: 7.844617841387681 and parameters: {'num_leaves': 17}. Best is trial 18 with value: 7.843588209976873.\u001b[0m\nnum_leaves, val_score: 7.843588:  70%|#######   | 14/20 [01:22<00:32,  5.43s/it]","output_type":"stream"},{"name":"stdout","text":"[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.119179 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 26263\n[LightGBM] [Info] Number of data points in the train set: 175000, number of used features: 103\n[LightGBM] [Info] Start training from score 1.920970\n","output_type":"stream"},{"name":"stderr","text":"num_leaves, val_score: 7.843588:  75%|#######5  | 15/20 [01:26<00:25,  5.01s/it]\u001b[32m[I 2021-08-25 21:53:25,796]\u001b[0m Trial 21 finished with value: 7.845534438387922 and parameters: {'num_leaves': 2}. Best is trial 18 with value: 7.843588209976873.\u001b[0m\nnum_leaves, val_score: 7.843588:  75%|#######5  | 15/20 [01:26<00:25,  5.01s/it]","output_type":"stream"},{"name":"stdout","text":"[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.119435 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 26263\n[LightGBM] [Info] Number of data points in the train set: 175000, number of used features: 103\n[LightGBM] [Info] Start training from score 1.920970\n","output_type":"stream"},{"name":"stderr","text":"num_leaves, val_score: 7.843588:  80%|########  | 16/20 [01:33<00:22,  5.60s/it]\u001b[32m[I 2021-08-25 21:53:32,776]\u001b[0m Trial 22 finished with value: 7.849865197924513 and parameters: {'num_leaves': 175}. Best is trial 18 with value: 7.843588209976873.\u001b[0m\nnum_leaves, val_score: 7.843588:  80%|########  | 16/20 [01:33<00:22,  5.60s/it]","output_type":"stream"},{"name":"stdout","text":"[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.122075 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 26263\n[LightGBM] [Info] Number of data points in the train set: 175000, number of used features: 103\n[LightGBM] [Info] Start training from score 1.920970\n","output_type":"stream"},{"name":"stderr","text":"num_leaves, val_score: 7.843588:  85%|########5 | 17/20 [01:38<00:16,  5.50s/it]\u001b[32m[I 2021-08-25 21:53:38,040]\u001b[0m Trial 23 finished with value: 7.845374243771136 and parameters: {'num_leaves': 47}. Best is trial 18 with value: 7.843588209976873.\u001b[0m\nnum_leaves, val_score: 7.843588:  85%|########5 | 17/20 [01:38<00:16,  5.50s/it]","output_type":"stream"},{"name":"stdout","text":"[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.123444 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 26263\n[LightGBM] [Info] Number of data points in the train set: 175000, number of used features: 103\n[LightGBM] [Info] Start training from score 1.920970\n","output_type":"stream"},{"name":"stderr","text":"num_leaves, val_score: 7.843588:  90%|######### | 18/20 [01:43<00:10,  5.43s/it]\u001b[32m[I 2021-08-25 21:53:43,287]\u001b[0m Trial 24 finished with value: 7.843749630379794 and parameters: {'num_leaves': 11}. Best is trial 18 with value: 7.843588209976873.\u001b[0m\nnum_leaves, val_score: 7.843588:  90%|######### | 18/20 [01:43<00:10,  5.43s/it]","output_type":"stream"},{"name":"stdout","text":"[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.117380 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 26263\n[LightGBM] [Info] Number of data points in the train set: 175000, number of used features: 103\n[LightGBM] [Info] Start training from score 1.920970\n","output_type":"stream"},{"name":"stderr","text":"num_leaves, val_score: 7.843588:  95%|#########5| 19/20 [01:50<00:05,  5.91s/it]\u001b[32m[I 2021-08-25 21:53:50,317]\u001b[0m Trial 25 finished with value: 7.846598742280958 and parameters: {'num_leaves': 101}. Best is trial 18 with value: 7.843588209976873.\u001b[0m\nnum_leaves, val_score: 7.843588:  95%|#########5| 19/20 [01:50<00:05,  5.91s/it]","output_type":"stream"},{"name":"stdout","text":"[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.117719 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 26263\n[LightGBM] [Info] Number of data points in the train set: 175000, number of used features: 103\n[LightGBM] [Info] Start training from score 1.920970\n","output_type":"stream"},{"name":"stderr","text":"num_leaves, val_score: 7.843588: 100%|##########| 20/20 [01:56<00:00,  5.75s/it]\u001b[32m[I 2021-08-25 21:53:55,709]\u001b[0m Trial 26 finished with value: 7.84569934145507 and parameters: {'num_leaves': 57}. Best is trial 18 with value: 7.843588209976873.\u001b[0m\nnum_leaves, val_score: 7.843588: 100%|##########| 20/20 [01:56<00:00,  5.81s/it]\nbagging, val_score: 7.843588:   0%|          | 0/10 [00:00<?, ?it/s]","output_type":"stream"},{"name":"stdout","text":"[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.121731 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 26263\n[LightGBM] [Info] Number of data points in the train set: 175000, number of used features: 103\n[LightGBM] [Info] Start training from score 1.920970\n","output_type":"stream"},{"name":"stderr","text":"bagging, val_score: 7.843588:  10%|#         | 1/10 [00:03<00:32,  3.66s/it]\u001b[32m[I 2021-08-25 21:53:59,379]\u001b[0m Trial 27 finished with value: 7.843734987337082 and parameters: {'bagging_fraction': 0.46272806926521437, 'bagging_freq': 6}. Best is trial 27 with value: 7.843734987337082.\u001b[0m\nbagging, val_score: 7.843588:  10%|#         | 1/10 [00:03<00:32,  3.66s/it]","output_type":"stream"},{"name":"stdout","text":"[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.118944 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 26263\n[LightGBM] [Info] Number of data points in the train set: 175000, number of used features: 103\n[LightGBM] [Info] Start training from score 1.920970\n","output_type":"stream"},{"name":"stderr","text":"bagging, val_score: 7.843588:  20%|##        | 2/10 [00:07<00:30,  3.79s/it]\u001b[32m[I 2021-08-25 21:54:03,266]\u001b[0m Trial 28 finished with value: 7.844649729679165 and parameters: {'bagging_fraction': 0.5624356576506784, 'bagging_freq': 2}. Best is trial 27 with value: 7.843734987337082.\u001b[0m\nbagging, val_score: 7.843588:  20%|##        | 2/10 [00:07<00:30,  3.79s/it]","output_type":"stream"},{"name":"stdout","text":"[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.123490 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 26263\n[LightGBM] [Info] Number of data points in the train set: 175000, number of used features: 103\n[LightGBM] [Info] Start training from score 1.920970\n","output_type":"stream"},{"name":"stderr","text":"bagging, val_score: 7.843588:  30%|###       | 3/10 [00:11<00:26,  3.80s/it]\u001b[32m[I 2021-08-25 21:54:07,068]\u001b[0m Trial 29 finished with value: 7.844395315242981 and parameters: {'bagging_fraction': 0.5303003208931194, 'bagging_freq': 1}. Best is trial 27 with value: 7.843734987337082.\u001b[0m\nbagging, val_score: 7.843588:  30%|###       | 3/10 [00:11<00:26,  3.80s/it]","output_type":"stream"},{"name":"stdout","text":"[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.118808 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 26263\n[LightGBM] [Info] Number of data points in the train set: 175000, number of used features: 103\n[LightGBM] [Info] Start training from score 1.920970\n","output_type":"stream"},{"name":"stderr","text":"bagging, val_score: 7.843588:  40%|####      | 4/10 [00:15<00:23,  3.99s/it]\u001b[32m[I 2021-08-25 21:54:11,345]\u001b[0m Trial 30 finished with value: 7.844672795376985 and parameters: {'bagging_fraction': 0.6689502134392584, 'bagging_freq': 3}. Best is trial 27 with value: 7.843734987337082.\u001b[0m\nbagging, val_score: 7.843588:  40%|####      | 4/10 [00:15<00:23,  3.99s/it]","output_type":"stream"},{"name":"stdout","text":"[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.120211 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 26263\n[LightGBM] [Info] Number of data points in the train set: 175000, number of used features: 103\n[LightGBM] [Info] Start training from score 1.920970\n","output_type":"stream"},{"name":"stderr","text":"bagging, val_score: 7.843588:  50%|#####     | 5/10 [00:19<00:19,  3.83s/it]\u001b[32m[I 2021-08-25 21:54:14,901]\u001b[0m Trial 31 finished with value: 7.844953413628611 and parameters: {'bagging_fraction': 0.4639346621942486, 'bagging_freq': 3}. Best is trial 27 with value: 7.843734987337082.\u001b[0m\nbagging, val_score: 7.843588:  50%|#####     | 5/10 [00:19<00:19,  3.83s/it]","output_type":"stream"},{"name":"stdout","text":"[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.115814 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 26263\n[LightGBM] [Info] Number of data points in the train set: 175000, number of used features: 103\n[LightGBM] [Info] Start training from score 1.920970\n","output_type":"stream"},{"name":"stderr","text":"bagging, val_score: 7.843588:  60%|######    | 6/10 [00:24<00:17,  4.31s/it]\u001b[32m[I 2021-08-25 21:54:20,134]\u001b[0m Trial 32 finished with value: 7.843968599824753 and parameters: {'bagging_fraction': 0.8311535125407127, 'bagging_freq': 6}. Best is trial 27 with value: 7.843734987337082.\u001b[0m\nbagging, val_score: 7.843588:  60%|######    | 6/10 [00:24<00:17,  4.31s/it]","output_type":"stream"},{"name":"stdout","text":"[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.119485 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 26263\n[LightGBM] [Info] Number of data points in the train set: 175000, number of used features: 103\n[LightGBM] [Info] Start training from score 1.920970\n","output_type":"stream"},{"name":"stderr","text":"bagging, val_score: 7.843588:  70%|#######   | 7/10 [00:27<00:12,  4.01s/it]\u001b[32m[I 2021-08-25 21:54:23,541]\u001b[0m Trial 33 finished with value: 7.845027591150585 and parameters: {'bagging_fraction': 0.43317312547631204, 'bagging_freq': 6}. Best is trial 27 with value: 7.843734987337082.\u001b[0m\nbagging, val_score: 7.843588:  70%|#######   | 7/10 [00:27<00:12,  4.01s/it]","output_type":"stream"},{"name":"stdout","text":"[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.116679 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 26263\n[LightGBM] [Info] Number of data points in the train set: 175000, number of used features: 103\n[LightGBM] [Info] Start training from score 1.920970\n","output_type":"stream"},{"name":"stderr","text":"bagging, val_score: 7.843588:  80%|########  | 8/10 [00:31<00:07,  3.94s/it]\u001b[32m[I 2021-08-25 21:54:27,318]\u001b[0m Trial 34 finished with value: 7.845651261131519 and parameters: {'bagging_fraction': 0.4228183006754546, 'bagging_freq': 5}. Best is trial 27 with value: 7.843734987337082.\u001b[0m\nbagging, val_score: 7.843588:  80%|########  | 8/10 [00:31<00:07,  3.94s/it]","output_type":"stream"},{"name":"stdout","text":"[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.119322 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 26263\n[LightGBM] [Info] Number of data points in the train set: 175000, number of used features: 103\n[LightGBM] [Info] Start training from score 1.920970\n","output_type":"stream"},{"name":"stderr","text":"bagging, val_score: 7.843379:  90%|######### | 9/10 [00:35<00:03,  3.99s/it]\u001b[32m[I 2021-08-25 21:54:31,416]\u001b[0m Trial 35 finished with value: 7.843379430503839 and parameters: {'bagging_fraction': 0.6664545812936506, 'bagging_freq': 5}. Best is trial 35 with value: 7.843379430503839.\u001b[0m\nbagging, val_score: 7.843379:  90%|######### | 9/10 [00:35<00:03,  3.99s/it]","output_type":"stream"},{"name":"stdout","text":"[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.114900 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 26263\n[LightGBM] [Info] Number of data points in the train set: 175000, number of used features: 103\n[LightGBM] [Info] Start training from score 1.920970\n","output_type":"stream"},{"name":"stderr","text":"bagging, val_score: 7.843379: 100%|##########| 10/10 [00:39<00:00,  3.82s/it]\u001b[32m[I 2021-08-25 21:54:34,862]\u001b[0m Trial 36 finished with value: 7.845849309272392 and parameters: {'bagging_fraction': 0.42284756396962647, 'bagging_freq': 6}. Best is trial 35 with value: 7.843379430503839.\u001b[0m\nbagging, val_score: 7.843379: 100%|##########| 10/10 [00:39<00:00,  3.91s/it]\nfeature_fraction_stage2, val_score: 7.843379:   0%|          | 0/3 [00:00<?, ?it/s]","output_type":"stream"},{"name":"stdout","text":"[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.129628 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 26263\n[LightGBM] [Info] Number of data points in the train set: 175000, number of used features: 103\n[LightGBM] [Info] Start training from score 1.920970\n","output_type":"stream"},{"name":"stderr","text":"feature_fraction_stage2, val_score: 7.843379:  33%|###3      | 1/3 [00:04<00:08,  4.36s/it]\u001b[32m[I 2021-08-25 21:54:39,229]\u001b[0m Trial 37 finished with value: 7.84403515495545 and parameters: {'feature_fraction': 0.92}. Best is trial 37 with value: 7.84403515495545.\u001b[0m\nfeature_fraction_stage2, val_score: 7.843379:  33%|###3      | 1/3 [00:04<00:08,  4.36s/it]","output_type":"stream"},{"name":"stdout","text":"[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.128217 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 26263\n[LightGBM] [Info] Number of data points in the train set: 175000, number of used features: 103\n[LightGBM] [Info] Start training from score 1.920970\n","output_type":"stream"},{"name":"stderr","text":"feature_fraction_stage2, val_score: 7.843379:  67%|######6   | 2/3 [00:08<00:04,  4.27s/it]\u001b[32m[I 2021-08-25 21:54:43,441]\u001b[0m Trial 38 finished with value: 7.843553410048589 and parameters: {'feature_fraction': 0.9520000000000001}. Best is trial 38 with value: 7.843553410048589.\u001b[0m\nfeature_fraction_stage2, val_score: 7.843379:  67%|######6   | 2/3 [00:08<00:04,  4.27s/it]","output_type":"stream"},{"name":"stdout","text":"[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.116529 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 26263\n[LightGBM] [Info] Number of data points in the train set: 175000, number of used features: 103\n[LightGBM] [Info] Start training from score 1.920970\n","output_type":"stream"},{"name":"stderr","text":"feature_fraction_stage2, val_score: 7.843348: 100%|##########| 3/3 [00:12<00:00,  4.22s/it]\u001b[32m[I 2021-08-25 21:54:47,594]\u001b[0m Trial 39 finished with value: 7.84334783909088 and parameters: {'feature_fraction': 0.9840000000000001}. Best is trial 39 with value: 7.84334783909088.\u001b[0m\nfeature_fraction_stage2, val_score: 7.843348: 100%|##########| 3/3 [00:12<00:00,  4.24s/it]\nregularization_factors, val_score: 7.843348:   0%|          | 0/20 [00:00<?, ?it/s]","output_type":"stream"},{"name":"stdout","text":"[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.117028 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 26263\n[LightGBM] [Info] Number of data points in the train set: 175000, number of used features: 103\n[LightGBM] [Info] Start training from score 1.920970\n","output_type":"stream"},{"name":"stderr","text":"regularization_factors, val_score: 7.843348:   5%|5         | 1/20 [00:04<01:34,  4.97s/it]\u001b[32m[I 2021-08-25 21:54:52,579]\u001b[0m Trial 40 finished with value: 7.843347838392612 and parameters: {'lambda_l1': 1.0349639520380658e-07, 'lambda_l2': 0.0012060801371631925}. Best is trial 40 with value: 7.843347838392612.\u001b[0m\nregularization_factors, val_score: 7.843348:   5%|5         | 1/20 [00:04<01:34,  4.97s/it]","output_type":"stream"},{"name":"stdout","text":"[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.112462 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 26263\n[LightGBM] [Info] Number of data points in the train set: 175000, number of used features: 103\n[LightGBM] [Info] Start training from score 1.920970\n","output_type":"stream"},{"name":"stderr","text":"regularization_factors, val_score: 7.843348:  10%|#         | 2/20 [00:09<01:20,  4.49s/it]\u001b[32m[I 2021-08-25 21:54:56,721]\u001b[0m Trial 41 finished with value: 7.843347839084653 and parameters: {'lambda_l1': 9.003974249497434e-07, 'lambda_l2': 9.928758565949167e-06}. Best is trial 40 with value: 7.843347838392612.\u001b[0m\nregularization_factors, val_score: 7.843348:  10%|#         | 2/20 [00:09<01:20,  4.49s/it]","output_type":"stream"},{"name":"stdout","text":"[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.044225 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 26263\n[LightGBM] [Info] Number of data points in the train set: 175000, number of used features: 103\n[LightGBM] [Info] Start training from score 1.920970\n","output_type":"stream"},{"name":"stderr","text":"regularization_factors, val_score: 7.843348:  15%|#5        | 3/20 [00:13<01:14,  4.39s/it]\u001b[32m[I 2021-08-25 21:55:00,988]\u001b[0m Trial 42 finished with value: 7.843347839088769 and parameters: {'lambda_l1': 1.6094654779775152e-06, 'lambda_l2': 1.401138753014734e-06}. Best is trial 40 with value: 7.843347838392612.\u001b[0m\nregularization_factors, val_score: 7.843348:  15%|#5        | 3/20 [00:13<01:14,  4.39s/it]","output_type":"stream"},{"name":"stdout","text":"[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.120586 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 26263\n[LightGBM] [Info] Number of data points in the train set: 175000, number of used features: 103\n[LightGBM] [Info] Start training from score 1.920970\n","output_type":"stream"},{"name":"stderr","text":"regularization_factors, val_score: 7.843348:  20%|##        | 4/20 [00:17<01:08,  4.28s/it]\u001b[32m[I 2021-08-25 21:55:05,118]\u001b[0m Trial 43 finished with value: 7.843476733054915 and parameters: {'lambda_l1': 1.2977674372208173e-06, 'lambda_l2': 3.561313542261352}. Best is trial 40 with value: 7.843347838392612.\u001b[0m\nregularization_factors, val_score: 7.843348:  20%|##        | 4/20 [00:17<01:08,  4.28s/it]","output_type":"stream"},{"name":"stdout","text":"[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.120622 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 26263\n[LightGBM] [Info] Number of data points in the train set: 175000, number of used features: 103\n[LightGBM] [Info] Start training from score 1.920970\n","output_type":"stream"},{"name":"stderr","text":"regularization_factors, val_score: 7.843322:  25%|##5       | 5/20 [00:21<01:03,  4.22s/it]\u001b[32m[I 2021-08-25 21:55:09,219]\u001b[0m Trial 44 finished with value: 7.843322442744274 and parameters: {'lambda_l1': 0.20709033266211635, 'lambda_l2': 0.14414494605683173}. Best is trial 44 with value: 7.843322442744274.\u001b[0m\nregularization_factors, val_score: 7.843322:  25%|##5       | 5/20 [00:21<01:03,  4.22s/it]","output_type":"stream"},{"name":"stdout","text":"[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.117176 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 26263\n[LightGBM] [Info] Number of data points in the train set: 175000, number of used features: 103\n[LightGBM] [Info] Start training from score 1.920970\n","output_type":"stream"},{"name":"stderr","text":"regularization_factors, val_score: 7.843182:  30%|###       | 6/20 [00:26<01:00,  4.33s/it]\u001b[32m[I 2021-08-25 21:55:13,771]\u001b[0m Trial 45 finished with value: 7.843182245563192 and parameters: {'lambda_l1': 4.031569351742911, 'lambda_l2': 0.08407656517043907}. Best is trial 45 with value: 7.843182245563192.\u001b[0m\nregularization_factors, val_score: 7.843182:  30%|###       | 6/20 [00:26<01:00,  4.33s/it]","output_type":"stream"},{"name":"stdout","text":"[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.112500 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 26263\n[LightGBM] [Info] Number of data points in the train set: 175000, number of used features: 103\n[LightGBM] [Info] Start training from score 1.920970\n","output_type":"stream"},{"name":"stderr","text":"regularization_factors, val_score: 7.843182:  35%|###5      | 7/20 [00:30<00:55,  4.25s/it]\u001b[32m[I 2021-08-25 21:55:17,868]\u001b[0m Trial 46 finished with value: 7.843347839066823 and parameters: {'lambda_l1': 1.1721280168563102e-06, 'lambda_l2': 4.118498140706821e-05}. Best is trial 45 with value: 7.843182245563192.\u001b[0m\nregularization_factors, val_score: 7.843182:  35%|###5      | 7/20 [00:30<00:55,  4.25s/it]","output_type":"stream"},{"name":"stdout","text":"[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.113660 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 26263\n[LightGBM] [Info] Number of data points in the train set: 175000, number of used features: 103\n[LightGBM] [Info] Start training from score 1.920970\n","output_type":"stream"},{"name":"stderr","text":"regularization_factors, val_score: 7.843182:  40%|####      | 8/20 [00:34<00:50,  4.24s/it]\u001b[32m[I 2021-08-25 21:55:22,065]\u001b[0m Trial 47 finished with value: 7.843347839090856 and parameters: {'lambda_l1': 1.1521754929247171e-08, 'lambda_l2': 2.4035125430270655e-08}. Best is trial 45 with value: 7.843182245563192.\u001b[0m\nregularization_factors, val_score: 7.843182:  40%|####      | 8/20 [00:34<00:50,  4.24s/it]","output_type":"stream"},{"name":"stdout","text":"[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.123439 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 26263\n[LightGBM] [Info] Number of data points in the train set: 175000, number of used features: 103\n[LightGBM] [Info] Start training from score 1.920970\n","output_type":"stream"},{"name":"stderr","text":"regularization_factors, val_score: 7.843182:  45%|####5     | 9/20 [00:38<00:47,  4.28s/it]\u001b[32m[I 2021-08-25 21:55:26,440]\u001b[0m Trial 48 finished with value: 7.843347839048369 and parameters: {'lambda_l1': 5.708262205005854e-07, 'lambda_l2': 7.504690154698701e-05}. Best is trial 45 with value: 7.843182245563192.\u001b[0m\nregularization_factors, val_score: 7.843182:  45%|####5     | 9/20 [00:38<00:47,  4.28s/it]","output_type":"stream"},{"name":"stdout","text":"[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.119558 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 26263\n[LightGBM] [Info] Number of data points in the train set: 175000, number of used features: 103\n[LightGBM] [Info] Start training from score 1.920970\n","output_type":"stream"},{"name":"stderr","text":"regularization_factors, val_score: 7.843105:  50%|#####     | 10/20 [00:43<00:43,  4.35s/it]\u001b[32m[I 2021-08-25 21:55:30,935]\u001b[0m Trial 49 finished with value: 7.843104549742184 and parameters: {'lambda_l1': 8.916988279459193, 'lambda_l2': 1.7943343419272821}. Best is trial 49 with value: 7.843104549742184.\u001b[0m\nregularization_factors, val_score: 7.843105:  50%|#####     | 10/20 [00:43<00:43,  4.35s/it]","output_type":"stream"},{"name":"stdout","text":"[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.114461 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 26263\n[LightGBM] [Info] Number of data points in the train set: 175000, number of used features: 103\n[LightGBM] [Info] Start training from score 1.920970\n","output_type":"stream"},{"name":"stderr","text":"regularization_factors, val_score: 7.843105:  55%|#####5    | 11/20 [00:47<00:38,  4.29s/it]\u001b[32m[I 2021-08-25 21:55:35,114]\u001b[0m Trial 50 finished with value: 7.84347401581913 and parameters: {'lambda_l1': 0.003970486343271129, 'lambda_l2': 7.52030195982807}. Best is trial 49 with value: 7.843104549742184.\u001b[0m\nregularization_factors, val_score: 7.843105:  55%|#####5    | 11/20 [00:47<00:38,  4.29s/it]","output_type":"stream"},{"name":"stdout","text":"[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.122849 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 26263\n[LightGBM] [Info] Number of data points in the train set: 175000, number of used features: 103\n[LightGBM] [Info] Start training from score 1.920970\n","output_type":"stream"},{"name":"stderr","text":"regularization_factors, val_score: 7.843105:  60%|######    | 12/20 [00:51<00:34,  4.25s/it]\u001b[32m[I 2021-08-25 21:55:39,276]\u001b[0m Trial 51 finished with value: 7.84373390116557 and parameters: {'lambda_l1': 7.725405944822913, 'lambda_l2': 0.04032975325392018}. Best is trial 49 with value: 7.843104549742184.\u001b[0m\nregularization_factors, val_score: 7.843105:  60%|######    | 12/20 [00:51<00:34,  4.25s/it]","output_type":"stream"},{"name":"stdout","text":"[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.116418 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 26263\n[LightGBM] [Info] Number of data points in the train set: 175000, number of used features: 103\n[LightGBM] [Info] Start training from score 1.920970\n","output_type":"stream"},{"name":"stderr","text":"regularization_factors, val_score: 7.843105:  65%|######5   | 13/20 [00:55<00:29,  4.25s/it]\u001b[32m[I 2021-08-25 21:55:43,519]\u001b[0m Trial 52 finished with value: 7.843492296916064 and parameters: {'lambda_l1': 3.31526368110332, 'lambda_l2': 0.09034831337914663}. Best is trial 49 with value: 7.843104549742184.\u001b[0m\nregularization_factors, val_score: 7.843105:  65%|######5   | 13/20 [00:55<00:29,  4.25s/it]","output_type":"stream"},{"name":"stdout","text":"[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.117188 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 26263\n[LightGBM] [Info] Number of data points in the train set: 175000, number of used features: 103\n[LightGBM] [Info] Start training from score 1.920970\n","output_type":"stream"},{"name":"stderr","text":"regularization_factors, val_score: 7.843105:  70%|#######   | 14/20 [01:00<00:25,  4.21s/it]\u001b[32m[I 2021-08-25 21:55:47,647]\u001b[0m Trial 53 finished with value: 7.843341335529264 and parameters: {'lambda_l1': 0.09742673829925742, 'lambda_l2': 0.0051376715941338115}. Best is trial 49 with value: 7.843104549742184.\u001b[0m\nregularization_factors, val_score: 7.843105:  70%|#######   | 14/20 [01:00<00:25,  4.21s/it]","output_type":"stream"},{"name":"stdout","text":"[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.115402 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 26263\n[LightGBM] [Info] Number of data points in the train set: 175000, number of used features: 103\n[LightGBM] [Info] Start training from score 1.920970\n","output_type":"stream"},{"name":"stderr","text":"regularization_factors, val_score: 7.843105:  75%|#######5  | 15/20 [01:04<00:20,  4.20s/it]\u001b[32m[I 2021-08-25 21:55:51,799]\u001b[0m Trial 54 finished with value: 7.843406090441431 and parameters: {'lambda_l1': 0.000299837419445509, 'lambda_l2': 1.326971147416678}. Best is trial 49 with value: 7.843104549742184.\u001b[0m\nregularization_factors, val_score: 7.843105:  75%|#######5  | 15/20 [01:04<00:20,  4.20s/it]","output_type":"stream"},{"name":"stdout","text":"[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.135490 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 26263\n[LightGBM] [Info] Number of data points in the train set: 175000, number of used features: 103\n[LightGBM] [Info] Start training from score 1.920970\n","output_type":"stream"},{"name":"stderr","text":"regularization_factors, val_score: 7.843105:  80%|########  | 16/20 [01:09<00:17,  4.46s/it]\u001b[32m[I 2021-08-25 21:55:56,863]\u001b[0m Trial 55 finished with value: 7.843445669708849 and parameters: {'lambda_l1': 0.26755850019222044, 'lambda_l2': 0.7606586788292324}. Best is trial 49 with value: 7.843104549742184.\u001b[0m\nregularization_factors, val_score: 7.843105:  80%|########  | 16/20 [01:09<00:17,  4.46s/it]","output_type":"stream"},{"name":"stdout","text":"[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.117681 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 26263\n[LightGBM] [Info] Number of data points in the train set: 175000, number of used features: 103\n[LightGBM] [Info] Start training from score 1.920970\n","output_type":"stream"},{"name":"stderr","text":"regularization_factors, val_score: 7.843105:  85%|########5 | 17/20 [01:13<00:13,  4.37s/it]\u001b[32m[I 2021-08-25 21:56:01,023]\u001b[0m Trial 56 finished with value: 7.843347835348515 and parameters: {'lambda_l1': 0.00196960564176857, 'lambda_l2': 0.004407044025006452}. Best is trial 49 with value: 7.843104549742184.\u001b[0m\nregularization_factors, val_score: 7.843105:  85%|########5 | 17/20 [01:13<00:13,  4.37s/it]","output_type":"stream"},{"name":"stdout","text":"[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.120854 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 26263\n[LightGBM] [Info] Number of data points in the train set: 175000, number of used features: 103\n[LightGBM] [Info] Start training from score 1.920970\n","output_type":"stream"},{"name":"stderr","text":"regularization_factors, val_score: 7.843105:  90%|######### | 18/20 [01:17<00:08,  4.37s/it]\u001b[32m[I 2021-08-25 21:56:05,384]\u001b[0m Trial 57 finished with value: 7.8437849873132075 and parameters: {'lambda_l1': 6.723927006401026, 'lambda_l2': 9.784385850564293}. Best is trial 49 with value: 7.843104549742184.\u001b[0m\nregularization_factors, val_score: 7.843105:  90%|######### | 18/20 [01:17<00:08,  4.37s/it]","output_type":"stream"},{"name":"stdout","text":"[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.117946 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 26263\n[LightGBM] [Info] Number of data points in the train set: 175000, number of used features: 103\n[LightGBM] [Info] Start training from score 1.920970\n","output_type":"stream"},{"name":"stderr","text":"regularization_factors, val_score: 7.843105:  95%|#########5| 19/20 [01:21<00:04,  4.29s/it]\u001b[32m[I 2021-08-25 21:56:09,503]\u001b[0m Trial 58 finished with value: 7.843347830666117 and parameters: {'lambda_l1': 4.753814626762312e-05, 'lambda_l2': 0.01434285974270274}. Best is trial 49 with value: 7.843104549742184.\u001b[0m\nregularization_factors, val_score: 7.843105:  95%|#########5| 19/20 [01:21<00:04,  4.29s/it]","output_type":"stream"},{"name":"stdout","text":"[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.116434 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 26263\n[LightGBM] [Info] Number of data points in the train set: 175000, number of used features: 103\n[LightGBM] [Info] Start training from score 1.920970\n","output_type":"stream"},{"name":"stderr","text":"regularization_factors, val_score: 7.843105: 100%|##########| 20/20 [01:26<00:00,  4.24s/it]\u001b[32m[I 2021-08-25 21:56:13,638]\u001b[0m Trial 59 finished with value: 7.843315863496836 and parameters: {'lambda_l1': 0.024786988505026347, 'lambda_l2': 0.6971958591472442}. Best is trial 49 with value: 7.843104549742184.\u001b[0m\nregularization_factors, val_score: 7.843105: 100%|##########| 20/20 [01:26<00:00,  4.30s/it]\nmin_data_in_leaf, val_score: 7.843105:   0%|          | 0/5 [00:00<?, ?it/s]","output_type":"stream"},{"name":"stdout","text":"[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.151493 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 26263\n[LightGBM] [Info] Number of data points in the train set: 175000, number of used features: 103\n[LightGBM] [Info] Start training from score 1.920970\n","output_type":"stream"},{"name":"stderr","text":"min_data_in_leaf, val_score: 7.843105:  20%|##        | 1/5 [00:04<00:16,  4.19s/it]\u001b[32m[I 2021-08-25 21:56:17,839]\u001b[0m Trial 60 finished with value: 7.843328969711378 and parameters: {'min_child_samples': 100}. Best is trial 60 with value: 7.843328969711378.\u001b[0m\nmin_data_in_leaf, val_score: 7.843105:  20%|##        | 1/5 [00:04<00:16,  4.19s/it]","output_type":"stream"},{"name":"stdout","text":"[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.117296 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 26263\n[LightGBM] [Info] Number of data points in the train set: 175000, number of used features: 103\n[LightGBM] [Info] Start training from score 1.920970\n","output_type":"stream"},{"name":"stderr","text":"min_data_in_leaf, val_score: 7.843105:  40%|####      | 2/5 [00:08<00:12,  4.19s/it]\u001b[32m[I 2021-08-25 21:56:22,029]\u001b[0m Trial 61 finished with value: 7.844207624437133 and parameters: {'min_child_samples': 5}. Best is trial 60 with value: 7.843328969711378.\u001b[0m\nmin_data_in_leaf, val_score: 7.843105:  40%|####      | 2/5 [00:08<00:12,  4.19s/it]","output_type":"stream"},{"name":"stdout","text":"[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.116926 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 26263\n[LightGBM] [Info] Number of data points in the train set: 175000, number of used features: 103\n[LightGBM] [Info] Start training from score 1.920970\n","output_type":"stream"},{"name":"stderr","text":"min_data_in_leaf, val_score: 7.843105:  60%|######    | 3/5 [00:12<00:08,  4.31s/it]\u001b[32m[I 2021-08-25 21:56:26,484]\u001b[0m Trial 62 finished with value: 7.843107333498706 and parameters: {'min_child_samples': 50}. Best is trial 62 with value: 7.843107333498706.\u001b[0m\nmin_data_in_leaf, val_score: 7.843105:  60%|######    | 3/5 [00:12<00:08,  4.31s/it]","output_type":"stream"},{"name":"stdout","text":"[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.117039 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 26263\n[LightGBM] [Info] Number of data points in the train set: 175000, number of used features: 103\n[LightGBM] [Info] Start training from score 1.920970\n","output_type":"stream"},{"name":"stderr","text":"min_data_in_leaf, val_score: 7.843105:  80%|########  | 4/5 [00:17<00:04,  4.40s/it]\u001b[32m[I 2021-08-25 21:56:31,007]\u001b[0m Trial 63 finished with value: 7.844046304660475 and parameters: {'min_child_samples': 10}. Best is trial 62 with value: 7.843107333498706.\u001b[0m\nmin_data_in_leaf, val_score: 7.843105:  80%|########  | 4/5 [00:17<00:04,  4.40s/it]","output_type":"stream"},{"name":"stdout","text":"[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.116101 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 26263\n[LightGBM] [Info] Number of data points in the train set: 175000, number of used features: 103\n[LightGBM] [Info] Start training from score 1.920970\n","output_type":"stream"},{"name":"stderr","text":"min_data_in_leaf, val_score: 7.843105: 100%|##########| 5/5 [00:21<00:00,  4.46s/it]\u001b[32m[I 2021-08-25 21:56:35,592]\u001b[0m Trial 64 finished with value: 7.843124478069344 and parameters: {'min_child_samples': 25}. Best is trial 62 with value: 7.843107333498706.\u001b[0m\nmin_data_in_leaf, val_score: 7.843105: 100%|##########| 5/5 [00:21<00:00,  4.39s/it]\n","output_type":"stream"},{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"{'objective': 'poisson',\n 'metric': 'rmse',\n 'feature_pre_filter': False,\n 'lambda_l1': 8.916988279459193,\n 'lambda_l2': 1.7943343419272821,\n 'num_leaves': 10,\n 'feature_fraction': 0.9840000000000001,\n 'bagging_fraction': 0.6664545812936506,\n 'bagging_freq': 5,\n 'min_child_samples': 20,\n 'num_iterations': 100,\n 'early_stopping_round': 5}"},"metadata":{}}]},{"cell_type":"code","source":"model_params = {'objective': 'poisson',\n                 'metric': 'rmse',\n                 'feature_pre_filter': False,\n                 'lambda_l1': 8.916988279459193,\n                 'lambda_l2': 1.7943343419272821,\n                 'num_leaves': 10,\n                 'feature_fraction': 0.9840000000000001,\n                 'bagging_fraction': 0.6664545812936506,\n                 'bagging_freq': 5,\n                 'min_child_samples': 20,\n                 'num_iterations': 100,\n                 'early_stopping_round': 5,\n               \"learning_rate\": 0.01,\n               \"num_iterations\":80000,\n               \"early_stopping_round\": 200}","metadata":{"execution":{"iopub.status.busy":"2021-08-25T22:22:19.838815Z","iopub.execute_input":"2021-08-25T22:22:19.839159Z","iopub.status.idle":"2021-08-25T22:22:19.845367Z","shell.execute_reply.started":"2021-08-25T22:22:19.839131Z","shell.execute_reply":"2021-08-25T22:22:19.844082Z"},"trusted":true},"execution_count":32,"outputs":[]},{"cell_type":"code","source":"final_preds = np.zeros(test.shape[0])\nfinal_train_oof = np.zeros(train.shape[0])\n\nkfd = KFold(n_splits=10, shuffle=True)\n\nfor fold, (train_idx, valid_idx) in enumerate(kfd.split(X=train, y=target)):\n    print(\"-\"*20, f\"FOLD {fold}\", \"-\"*20)\n    X_train, X_valid = train.iloc[train_idx], train.iloc[valid_idx]\n    y_train, y_valid = target.iloc[train_idx], target.iloc[valid_idx]\n    \n    train_dataset = lgb.Dataset(X_train, y_train)\n    valid_dataset = lgb.Dataset(X_valid, y_valid)\n    \n    model = lgb.train(model_params, train_dataset, valid_sets=[valid_dataset], verbose_eval=1000)\n    \n    final_train_oof[valid_idx] = model.predict(X_valid)\n    final_preds += model.predict(test) / 10","metadata":{"execution":{"iopub.status.busy":"2021-08-25T22:22:28.438718Z","iopub.execute_input":"2021-08-25T22:22:28.439597Z","iopub.status.idle":"2021-08-25T22:29:26.311505Z","shell.execute_reply.started":"2021-08-25T22:22:28.439541Z","shell.execute_reply":"2021-08-25T22:29:26.310599Z"},"trusted":true},"execution_count":33,"outputs":[{"name":"stdout","text":"-------------------- FOLD 0 --------------------\n[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.138454 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 26264\n[LightGBM] [Info] Number of data points in the train set: 225000, number of used features: 103\n[LightGBM] [Info] Start training from score 1.920243\nTraining until validation scores don't improve for 200 rounds\nEarly stopping, best iteration is:\n[660]\tvalid_0's rmse: 7.80335\n-------------------- FOLD 1 --------------------\n[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.139654 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 26263\n[LightGBM] [Info] Number of data points in the train set: 225000, number of used features: 103\n[LightGBM] [Info] Start training from score 1.919283\nTraining until validation scores don't improve for 200 rounds\n[1000]\tvalid_0's rmse: 7.80404\nEarly stopping, best iteration is:\n[955]\tvalid_0's rmse: 7.80381\n-------------------- FOLD 2 --------------------\n[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.140613 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 26264\n[LightGBM] [Info] Number of data points in the train set: 225000, number of used features: 103\n[LightGBM] [Info] Start training from score 1.918405\nTraining until validation scores don't improve for 200 rounds\nEarly stopping, best iteration is:\n[745]\tvalid_0's rmse: 7.82663\n-------------------- FOLD 3 --------------------\n[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.140560 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 26264\n[LightGBM] [Info] Number of data points in the train set: 225000, number of used features: 103\n[LightGBM] [Info] Start training from score 1.918147\nTraining until validation scores don't improve for 200 rounds\n[1000]\tvalid_0's rmse: 7.84972\nEarly stopping, best iteration is:\n[1045]\tvalid_0's rmse: 7.84969\n-------------------- FOLD 4 --------------------\n[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.140773 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 26264\n[LightGBM] [Info] Number of data points in the train set: 225000, number of used features: 103\n[LightGBM] [Info] Start training from score 1.918996\nTraining until validation scores don't improve for 200 rounds\nEarly stopping, best iteration is:\n[625]\tvalid_0's rmse: 7.83213\n-------------------- FOLD 5 --------------------\n[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.140782 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 26264\n[LightGBM] [Info] Number of data points in the train set: 225000, number of used features: 103\n[LightGBM] [Info] Start training from score 1.919370\nTraining until validation scores don't improve for 200 rounds\nEarly stopping, best iteration is:\n[735]\tvalid_0's rmse: 7.79326\n-------------------- FOLD 6 --------------------\n[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.039311 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 26263\n[LightGBM] [Info] Number of data points in the train set: 225000, number of used features: 103\n[LightGBM] [Info] Start training from score 1.917856\nTraining until validation scores don't improve for 200 rounds\n[1000]\tvalid_0's rmse: 7.89071\nEarly stopping, best iteration is:\n[874]\tvalid_0's rmse: 7.89039\n-------------------- FOLD 7 --------------------\n[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.140137 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 26262\n[LightGBM] [Info] Number of data points in the train set: 225000, number of used features: 103\n[LightGBM] [Info] Start training from score 1.919179\nTraining until validation scores don't improve for 200 rounds\n[1000]\tvalid_0's rmse: 7.86592\nEarly stopping, best iteration is:\n[1090]\tvalid_0's rmse: 7.86569\n-------------------- FOLD 8 --------------------\n[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.140724 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 26264\n[LightGBM] [Info] Number of data points in the train set: 225000, number of used features: 103\n[LightGBM] [Info] Start training from score 1.919133\nTraining until validation scores don't improve for 200 rounds\n[1000]\tvalid_0's rmse: 7.86288\nEarly stopping, best iteration is:\n[940]\tvalid_0's rmse: 7.86275\n-------------------- FOLD 9 --------------------\n[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.039737 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 26264\n[LightGBM] [Info] Number of data points in the train set: 225000, number of used features: 103\n[LightGBM] [Info] Start training from score 1.919063\nTraining until validation scores don't improve for 200 rounds\nEarly stopping, best iteration is:\n[711]\tvalid_0's rmse: 7.84777\n","output_type":"stream"}]},{"cell_type":"code","source":"predictions = pd.DataFrame()\npredictions[\"id\"] = test.index\npredictions[\"loss\"] = final_preds\n\npredictions.to_csv('final_submission4.csv', index=False, header=predictions.columns)","metadata":{"execution":{"iopub.status.busy":"2021-08-25T22:30:23.971782Z","iopub.execute_input":"2021-08-25T22:30:23.972111Z","iopub.status.idle":"2021-08-25T22:30:24.531459Z","shell.execute_reply.started":"2021-08-25T22:30:23.972083Z","shell.execute_reply":"2021-08-25T22:30:24.530628Z"},"trusted":true},"execution_count":35,"outputs":[]},{"cell_type":"code","source":"predictions = pd.DataFrame()\npredictions[\"id\"] = train.index\npredictions[\"loss\"] = final_train_oof\n\npredictions.to_csv('final_train_oof.csv', index=False, header=predictions.columns)","metadata":{"execution":{"iopub.status.busy":"2021-08-25T22:31:08.708905Z","iopub.execute_input":"2021-08-25T22:31:08.709409Z","iopub.status.idle":"2021-08-25T22:31:09.620545Z","shell.execute_reply.started":"2021-08-25T22:31:08.709365Z","shell.execute_reply":"2021-08-25T22:31:09.619681Z"},"trusted":true},"execution_count":39,"outputs":[]}]}