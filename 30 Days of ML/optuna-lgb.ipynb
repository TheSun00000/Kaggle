{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-08-26T00:38:21.859692Z","iopub.execute_input":"2021-08-26T00:38:21.860055Z","iopub.status.idle":"2021-08-26T00:38:21.883087Z","shell.execute_reply.started":"2021-08-26T00:38:21.860024Z","shell.execute_reply":"2021-08-26T00:38:21.882125Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"/kaggle/input/30-days-of-ml/sample_submission.csv\n/kaggle/input/30-days-of-ml/train.csv\n/kaggle/input/30-days-of-ml/test.csv\n/kaggle/input/notebook92c67b5720/submission1.1.csv\n/kaggle/input/notebook92c67b5720/submission0.3.csv\n/kaggle/input/notebook92c67b5720/submission0.5.csv\n/kaggle/input/notebook92c67b5720/submission1.7.csv\n/kaggle/input/notebook92c67b5720/submission0.7.csv\n/kaggle/input/notebook92c67b5720/__results__.html\n/kaggle/input/notebook92c67b5720/submission0.7+ctb.csv\n/kaggle/input/notebook92c67b5720/submission1.csv\n/kaggle/input/notebook92c67b5720/__notebook_source__.ipynb\n/kaggle/input/notebook92c67b5720/submission1.3.csv\n/kaggle/input/notebook92c67b5720/__notebook__.ipynb\n/kaggle/input/notebook92c67b5720/__output__.json\n/kaggle/input/notebook92c67b5720/xgb_lgb_ctb_oof\n/kaggle/input/notebook92c67b5720/submission0.3+ctb.csv\n/kaggle/input/notebook92c67b5720/custom.css\n/kaggle/input/notebook92c67b5720/__results___files/__results___8_1.png\n/kaggle/input/notebook92c67b5720/catboost_info/test_error.tsv\n/kaggle/input/notebook92c67b5720/catboost_info/learn_error.tsv\n/kaggle/input/notebook92c67b5720/catboost_info/catboost_training.json\n/kaggle/input/notebook92c67b5720/catboost_info/time_left.tsv\n/kaggle/input/notebook92c67b5720/catboost_info/learn/events.out.tfevents\n/kaggle/input/notebook92c67b5720/catboost_info/test/events.out.tfevents\n/kaggle/input/optuna-lgb/final_submission4.csv\n/kaggle/input/optuna-lgb/final_submission.csv\n/kaggle/input/optuna-lgb/final_train_oof.csv\n/kaggle/input/optuna-lgb/__results__.html\n/kaggle/input/optuna-lgb/__notebook_source__.ipynb\n/kaggle/input/optuna-lgb/__notebook__.ipynb\n/kaggle/input/optuna-lgb/__output__.json\n/kaggle/input/optuna-lgb/custom.css\n","output_type":"stream"}]},{"cell_type":"code","source":"import sklearn\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.preprocessing import OneHotEncoder, OrdinalEncoder\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport xgboost as xgb\nimport catboost as ctb\nimport lightgbm as lgb\nimport random\nimport os\n\nfrom sklearn.model_selection import KFold, train_test_split\nfrom sklearn.linear_model import LinearRegression, ElasticNet, BayesianRidge, Lasso\nfrom sklearn.preprocessing import StandardScaler","metadata":{"execution":{"iopub.status.busy":"2021-08-26T00:38:52.538547Z","iopub.execute_input":"2021-08-26T00:38:52.538895Z","iopub.status.idle":"2021-08-26T00:38:52.545901Z","shell.execute_reply.started":"2021-08-26T00:38:52.538864Z","shell.execute_reply":"2021-08-26T00:38:52.544946Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"train = pd.read_csv(\"/kaggle/input/30-days-of-ml/train.csv\", index_col=\"id\")\ntest  = pd.read_csv(\"/kaggle/input/30-days-of-ml/test.csv\", index_col=\"id\")\n\ntarget = train[\"target\"]\ntrain = train.drop(\"target\", axis=1)","metadata":{"execution":{"iopub.status.busy":"2021-08-26T00:38:53.908870Z","iopub.execute_input":"2021-08-26T00:38:53.911202Z","iopub.status.idle":"2021-08-26T00:38:57.077215Z","shell.execute_reply.started":"2021-08-26T00:38:53.911119Z","shell.execute_reply":"2021-08-26T00:38:57.075908Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"train_add = pd.read_csv(\"/kaggle/input/notebook92c67b5720/xgb_lgb_ctb_oof\")\ntrain_add[\"id\"] = train.index\ntrain_add = train_add.set_index(\"id\")\ntest_add  = pd.read_csv(\"/kaggle/input/notebook92c67b5720/submission1.csv\").set_index(\"id\")","metadata":{"execution":{"iopub.status.busy":"2021-08-26T00:38:57.079521Z","iopub.execute_input":"2021-08-26T00:38:57.080075Z","iopub.status.idle":"2021-08-26T00:38:57.549788Z","shell.execute_reply.started":"2021-08-26T00:38:57.080001Z","shell.execute_reply":"2021-08-26T00:38:57.548958Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"train = pd.concat([train, train_add],axis=1)\ntest  = pd.concat([test , test_add ],axis=1).rename(columns={\"target\":\"oof\"})","metadata":{"execution":{"iopub.status.busy":"2021-08-26T00:38:57.551421Z","iopub.execute_input":"2021-08-26T00:38:57.551810Z","iopub.status.idle":"2021-08-26T00:38:57.644145Z","shell.execute_reply.started":"2021-08-26T00:38:57.551773Z","shell.execute_reply":"2021-08-26T00:38:57.643241Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"train[\"oof\"] = 0.5*train[\"xgb\"] + 0.5*train[\"lgb\"]\ntrain = train.drop([\"xgb\", \"lgb\", \"ctb\"], axis=1)","metadata":{"execution":{"iopub.status.busy":"2021-08-26T00:39:03.392172Z","iopub.execute_input":"2021-08-26T00:39:03.392483Z","iopub.status.idle":"2021-08-26T00:39:03.462048Z","shell.execute_reply.started":"2021-08-26T00:39:03.392453Z","shell.execute_reply":"2021-08-26T00:39:03.461176Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"Count_cols   = train.select_dtypes(exclude=['object']).columns\nCat_cols = [ col for col in train.columns if col not in Count_cols ]\nprint(\"Number of columns:\", len(train.columns))\nprint(\"Number of catrgorical columns:\", len(Cat_cols))\nprint(\"Number of continuous columns:\",  len(Count_cols))\n\nordinalencoder = OrdinalEncoder()\n\nOH_cat_train = pd.DataFrame(ordinalencoder.fit_transform(train[Cat_cols]))\nOH_cat_test = pd.DataFrame(ordinalencoder.transform(test[Cat_cols]))\n\nOH_cat_train.index = train.index\nOH_cat_test.index = test.index\n\nOH_num_train = train[Count_cols]\nOH_num_test = test[Count_cols]\n\ntrain   = pd.concat([OH_cat_train, OH_num_train], axis=1)\ntest   = pd.concat([OH_cat_test, OH_num_test], axis=1)","metadata":{"execution":{"iopub.status.busy":"2021-08-26T00:39:04.903264Z","iopub.execute_input":"2021-08-26T00:39:04.903576Z","iopub.status.idle":"2021-08-26T00:39:06.694011Z","shell.execute_reply.started":"2021-08-26T00:39:04.903546Z","shell.execute_reply":"2021-08-26T00:39:06.693095Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"Number of columns: 25\nNumber of catrgorical columns: 10\nNumber of continuous columns: 15\n","output_type":"stream"}]},{"cell_type":"code","source":"import optuna \nimport optuna.integration.lightgbm as lgbo\n\nparams = {'objective': 'poisson',  'metric': 'rmse' } #'objective': 'mean_squared_error',\nfrom sklearn.metrics import mean_absolute_error\nx_train, x_test, y_train, y_test = train_test_split(train, target, test_size=0.3, random_state=42)\nlgb_train = lgb.Dataset(x_train, y_train)\nlgb_valid = lgb.Dataset(x_test, y_test)\nmodel = lgbo.train(params,\n                   lgb_train,\n                   valid_sets=[lgb_valid],\n                   verbose_eval=False,\n                   num_boost_round=100,\n                   early_stopping_rounds=5,) \nmodel.params","metadata":{"execution":{"iopub.status.busy":"2021-08-25T23:55:53.401483Z","iopub.execute_input":"2021-08-25T23:55:53.401862Z","iopub.status.idle":"2021-08-25T23:58:08.319927Z","shell.execute_reply.started":"2021-08-25T23:55:53.401827Z","shell.execute_reply":"2021-08-25T23:58:08.319113Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":57,"outputs":[{"name":"stderr","text":"\u001b[32m[I 2021-08-25 23:55:54,174]\u001b[0m A new study created in memory with name: no-name-6fa58784-540b-4021-9e9a-182935a8171b\u001b[0m\nfeature_fraction, val_score: inf:   0%|          | 0/7 [00:00<?, ?it/s]","output_type":"stream"},{"name":"stdout","text":"[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.016227 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 3881\n[LightGBM] [Info] Number of data points in the train set: 210000, number of used features: 25\n[LightGBM] [Info] Start training from score 2.109188\n","output_type":"stream"},{"name":"stderr","text":"feature_fraction, val_score: 0.718147:  14%|#4        | 1/7 [00:02<00:12,  2.08s/it]\u001b[32m[I 2021-08-25 23:55:56,260]\u001b[0m Trial 0 finished with value: 0.7181470642434248 and parameters: {'feature_fraction': 0.4}. Best is trial 0 with value: 0.7181470642434248.\u001b[0m\nfeature_fraction, val_score: 0.718147:  14%|#4        | 1/7 [00:02<00:12,  2.08s/it]","output_type":"stream"},{"name":"stdout","text":"[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.014372 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 3881\n[LightGBM] [Info] Number of data points in the train set: 210000, number of used features: 25\n[LightGBM] [Info] Start training from score 2.109188\n","output_type":"stream"},{"name":"stderr","text":"feature_fraction, val_score: 0.717689:  29%|##8       | 2/7 [00:04<00:10,  2.10s/it]\u001b[32m[I 2021-08-25 23:55:58,370]\u001b[0m Trial 1 finished with value: 0.7176890938094801 and parameters: {'feature_fraction': 0.5}. Best is trial 1 with value: 0.7176890938094801.\u001b[0m\nfeature_fraction, val_score: 0.717689:  29%|##8       | 2/7 [00:04<00:10,  2.10s/it]","output_type":"stream"},{"name":"stdout","text":"[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.039472 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 3881\n[LightGBM] [Info] Number of data points in the train set: 210000, number of used features: 25\n[LightGBM] [Info] Start training from score 2.109188\n","output_type":"stream"},{"name":"stderr","text":"feature_fraction, val_score: 0.717660:  43%|####2     | 3/7 [00:06<00:08,  2.11s/it]\u001b[32m[I 2021-08-25 23:56:00,498]\u001b[0m Trial 2 finished with value: 0.7176599500393293 and parameters: {'feature_fraction': 0.6}. Best is trial 2 with value: 0.7176599500393293.\u001b[0m\nfeature_fraction, val_score: 0.717660:  43%|####2     | 3/7 [00:06<00:08,  2.11s/it]","output_type":"stream"},{"name":"stdout","text":"[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.040342 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 3881\n[LightGBM] [Info] Number of data points in the train set: 210000, number of used features: 25\n[LightGBM] [Info] Start training from score 2.109188\n","output_type":"stream"},{"name":"stderr","text":"feature_fraction, val_score: 0.717190:  57%|#####7    | 4/7 [00:08<00:06,  2.14s/it]\u001b[32m[I 2021-08-25 23:56:02,682]\u001b[0m Trial 3 finished with value: 0.7171903016223771 and parameters: {'feature_fraction': 0.8}. Best is trial 3 with value: 0.7171903016223771.\u001b[0m\nfeature_fraction, val_score: 0.717190:  57%|#####7    | 4/7 [00:08<00:06,  2.14s/it]","output_type":"stream"},{"name":"stdout","text":"[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.040675 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 3881\n[LightGBM] [Info] Number of data points in the train set: 210000, number of used features: 25\n[LightGBM] [Info] Start training from score 2.109188\n","output_type":"stream"},{"name":"stderr","text":"feature_fraction, val_score: 0.717190:  71%|#######1  | 5/7 [00:10<00:04,  2.16s/it]\u001b[32m[I 2021-08-25 23:56:04,874]\u001b[0m Trial 4 finished with value: 0.7172856893972898 and parameters: {'feature_fraction': 0.7}. Best is trial 3 with value: 0.7171903016223771.\u001b[0m\nfeature_fraction, val_score: 0.717190:  71%|#######1  | 5/7 [00:10<00:04,  2.16s/it]","output_type":"stream"},{"name":"stdout","text":"[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.017480 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 3881\n[LightGBM] [Info] Number of data points in the train set: 210000, number of used features: 25\n[LightGBM] [Info] Start training from score 2.109188\n","output_type":"stream"},{"name":"stderr","text":"feature_fraction, val_score: 0.716904:  86%|########5 | 6/7 [00:12<00:02,  2.06s/it]\u001b[32m[I 2021-08-25 23:56:06,743]\u001b[0m Trial 5 finished with value: 0.7169037365966628 and parameters: {'feature_fraction': 1.0}. Best is trial 5 with value: 0.7169037365966628.\u001b[0m\nfeature_fraction, val_score: 0.716904:  86%|########5 | 6/7 [00:12<00:02,  2.06s/it]","output_type":"stream"},{"name":"stdout","text":"[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.039917 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 3881\n[LightGBM] [Info] Number of data points in the train set: 210000, number of used features: 25\n[LightGBM] [Info] Start training from score 2.109188\n","output_type":"stream"},{"name":"stderr","text":"feature_fraction, val_score: 0.716904: 100%|##########| 7/7 [00:14<00:00,  2.02s/it]\u001b[32m[I 2021-08-25 23:56:08,674]\u001b[0m Trial 6 finished with value: 0.7170757368620054 and parameters: {'feature_fraction': 0.8999999999999999}. Best is trial 5 with value: 0.7169037365966628.\u001b[0m\nfeature_fraction, val_score: 0.716904: 100%|##########| 7/7 [00:14<00:00,  2.07s/it]\nnum_leaves, val_score: 0.716904:   0%|          | 0/20 [00:00<?, ?it/s]","output_type":"stream"},{"name":"stdout","text":"[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.018200 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 3881\n[LightGBM] [Info] Number of data points in the train set: 210000, number of used features: 25\n[LightGBM] [Info] Start training from score 2.109188\n","output_type":"stream"},{"name":"stderr","text":"num_leaves, val_score: 0.716734:   5%|5         | 1/20 [00:01<00:33,  1.77s/it]\u001b[32m[I 2021-08-25 23:56:10,452]\u001b[0m Trial 7 finished with value: 0.7167337441934575 and parameters: {'num_leaves': 13}. Best is trial 7 with value: 0.7167337441934575.\u001b[0m\nnum_leaves, val_score: 0.716734:   5%|5         | 1/20 [00:01<00:33,  1.77s/it]","output_type":"stream"},{"name":"stdout","text":"[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.019258 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 3881\n[LightGBM] [Info] Number of data points in the train set: 210000, number of used features: 25\n[LightGBM] [Info] Start training from score 2.109188\n","output_type":"stream"},{"name":"stderr","text":"num_leaves, val_score: 0.716734:  10%|#         | 2/20 [00:03<00:35,  1.96s/it]\u001b[32m[I 2021-08-25 23:56:12,536]\u001b[0m Trial 8 finished with value: 0.7171075124738194 and parameters: {'num_leaves': 74}. Best is trial 7 with value: 0.7167337441934575.\u001b[0m\nnum_leaves, val_score: 0.716734:  10%|#         | 2/20 [00:03<00:35,  1.96s/it]","output_type":"stream"},{"name":"stdout","text":"[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.060558 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 3881\n[LightGBM] [Info] Number of data points in the train set: 210000, number of used features: 25\n[LightGBM] [Info] Start training from score 2.109188\n","output_type":"stream"},{"name":"stderr","text":"num_leaves, val_score: 0.716734:  15%|#5        | 3/20 [00:06<00:39,  2.35s/it]\u001b[32m[I 2021-08-25 23:56:15,360]\u001b[0m Trial 9 finished with value: 0.7175098885125072 and parameters: {'num_leaves': 221}. Best is trial 7 with value: 0.7167337441934575.\u001b[0m\nnum_leaves, val_score: 0.716734:  15%|#5        | 3/20 [00:06<00:39,  2.35s/it]","output_type":"stream"},{"name":"stdout","text":"[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.017361 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 3881\n[LightGBM] [Info] Number of data points in the train set: 210000, number of used features: 25\n[LightGBM] [Info] Start training from score 2.109188\n","output_type":"stream"},{"name":"stderr","text":"num_leaves, val_score: 0.716734:  20%|##        | 4/20 [00:09<00:38,  2.39s/it]\u001b[32m[I 2021-08-25 23:56:17,805]\u001b[0m Trial 10 finished with value: 0.7174527912754055 and parameters: {'num_leaves': 160}. Best is trial 7 with value: 0.7167337441934575.\u001b[0m\nnum_leaves, val_score: 0.716734:  20%|##        | 4/20 [00:09<00:38,  2.39s/it]","output_type":"stream"},{"name":"stdout","text":"[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.017710 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 3881\n[LightGBM] [Info] Number of data points in the train set: 210000, number of used features: 25\n[LightGBM] [Info] Start training from score 2.109188\n","output_type":"stream"},{"name":"stderr","text":"num_leaves, val_score: 0.716734:  25%|##5       | 5/20 [00:12<00:38,  2.58s/it]\u001b[32m[I 2021-08-25 23:56:20,731]\u001b[0m Trial 11 finished with value: 0.7171661415155798 and parameters: {'num_leaves': 76}. Best is trial 7 with value: 0.7167337441934575.\u001b[0m\nnum_leaves, val_score: 0.716734:  25%|##5       | 5/20 [00:12<00:38,  2.58s/it]","output_type":"stream"},{"name":"stdout","text":"[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.017029 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 3881\n[LightGBM] [Info] Number of data points in the train set: 210000, number of used features: 25\n[LightGBM] [Info] Start training from score 2.109188\n","output_type":"stream"},{"name":"stderr","text":"num_leaves, val_score: 0.716734:  30%|###       | 6/20 [00:14<00:36,  2.58s/it]\u001b[32m[I 2021-08-25 23:56:23,320]\u001b[0m Trial 12 finished with value: 0.717520467167132 and parameters: {'num_leaves': 207}. Best is trial 7 with value: 0.7167337441934575.\u001b[0m\nnum_leaves, val_score: 0.716734:  30%|###       | 6/20 [00:14<00:36,  2.58s/it]","output_type":"stream"},{"name":"stdout","text":"[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.018247 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 3881\n[LightGBM] [Info] Number of data points in the train set: 210000, number of used features: 25\n[LightGBM] [Info] Start training from score 2.109188\n","output_type":"stream"},{"name":"stderr","text":"num_leaves, val_score: 0.716734:  35%|###5      | 7/20 [00:16<00:30,  2.37s/it]\u001b[32m[I 2021-08-25 23:56:25,239]\u001b[0m Trial 13 finished with value: 0.7168861369382793 and parameters: {'num_leaves': 29}. Best is trial 7 with value: 0.7167337441934575.\u001b[0m\nnum_leaves, val_score: 0.716734:  35%|###5      | 7/20 [00:16<00:30,  2.37s/it]","output_type":"stream"},{"name":"stdout","text":"[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.016695 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 3881\n[LightGBM] [Info] Number of data points in the train set: 210000, number of used features: 25\n[LightGBM] [Info] Start training from score 2.109188\n","output_type":"stream"},{"name":"stderr","text":"num_leaves, val_score: 0.716734:  40%|####      | 8/20 [00:19<00:28,  2.39s/it]\u001b[32m[I 2021-08-25 23:56:27,684]\u001b[0m Trial 14 finished with value: 0.7176208053880512 and parameters: {'num_leaves': 211}. Best is trial 7 with value: 0.7167337441934575.\u001b[0m\nnum_leaves, val_score: 0.716734:  40%|####      | 8/20 [00:19<00:28,  2.39s/it]","output_type":"stream"},{"name":"stdout","text":"[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.017053 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 3881\n[LightGBM] [Info] Number of data points in the train set: 210000, number of used features: 25\n[LightGBM] [Info] Start training from score 2.109188\n","output_type":"stream"},{"name":"stderr","text":"num_leaves, val_score: 0.716734:  45%|####5     | 9/20 [00:21<00:25,  2.34s/it]\u001b[32m[I 2021-08-25 23:56:29,920]\u001b[0m Trial 15 finished with value: 0.7174135861791989 and parameters: {'num_leaves': 162}. Best is trial 7 with value: 0.7167337441934575.\u001b[0m\nnum_leaves, val_score: 0.716734:  45%|####5     | 9/20 [00:21<00:25,  2.34s/it]","output_type":"stream"},{"name":"stdout","text":"[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.017455 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 3881\n[LightGBM] [Info] Number of data points in the train set: 210000, number of used features: 25\n[LightGBM] [Info] Start training from score 2.109188\n","output_type":"stream"},{"name":"stderr","text":"num_leaves, val_score: 0.716734:  50%|#####     | 10/20 [00:24<00:24,  2.49s/it]\u001b[32m[I 2021-08-25 23:56:32,728]\u001b[0m Trial 16 finished with value: 0.71768918258922 and parameters: {'num_leaves': 252}. Best is trial 7 with value: 0.7167337441934575.\u001b[0m\nnum_leaves, val_score: 0.716734:  50%|#####     | 10/20 [00:24<00:24,  2.49s/it]","output_type":"stream"},{"name":"stdout","text":"[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.016990 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 3881\n[LightGBM] [Info] Number of data points in the train set: 210000, number of used features: 25\n[LightGBM] [Info] Start training from score 2.109188\n","output_type":"stream"},{"name":"stderr","text":"num_leaves, val_score: 0.716734:  55%|#####5    | 11/20 [00:25<00:19,  2.17s/it]\u001b[32m[I 2021-08-25 23:56:34,180]\u001b[0m Trial 17 finished with value: 0.717059599302166 and parameters: {'num_leaves': 2}. Best is trial 7 with value: 0.7167337441934575.\u001b[0m\nnum_leaves, val_score: 0.716734:  55%|#####5    | 11/20 [00:25<00:19,  2.17s/it]","output_type":"stream"},{"name":"stdout","text":"[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.017729 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 3881\n[LightGBM] [Info] Number of data points in the train set: 210000, number of used features: 25\n[LightGBM] [Info] Start training from score 2.109188\n","output_type":"stream"},{"name":"stderr","text":"num_leaves, val_score: 0.716734:  60%|######    | 12/20 [00:27<00:16,  2.10s/it]\u001b[32m[I 2021-08-25 23:56:36,109]\u001b[0m Trial 18 finished with value: 0.716819501751355 and parameters: {'num_leaves': 17}. Best is trial 7 with value: 0.7167337441934575.\u001b[0m\nnum_leaves, val_score: 0.716734:  60%|######    | 12/20 [00:27<00:16,  2.10s/it]","output_type":"stream"},{"name":"stdout","text":"[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.017386 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 3881\n[LightGBM] [Info] Number of data points in the train set: 210000, number of used features: 25\n[LightGBM] [Info] Start training from score 2.109188\n","output_type":"stream"},{"name":"stderr","text":"num_leaves, val_score: 0.716734:  65%|######5   | 13/20 [00:28<00:13,  1.91s/it]\u001b[32m[I 2021-08-25 23:56:37,574]\u001b[0m Trial 19 finished with value: 0.717059599302166 and parameters: {'num_leaves': 2}. Best is trial 7 with value: 0.7167337441934575.\u001b[0m\nnum_leaves, val_score: 0.716734:  65%|######5   | 13/20 [00:28<00:13,  1.91s/it]","output_type":"stream"},{"name":"stdout","text":"[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.018296 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 3881\n[LightGBM] [Info] Number of data points in the train set: 210000, number of used features: 25\n[LightGBM] [Info] Start training from score 2.109188\n","output_type":"stream"},{"name":"stderr","text":"num_leaves, val_score: 0.716734:  70%|#######   | 14/20 [00:30<00:11,  1.92s/it]\u001b[32m[I 2021-08-25 23:56:39,515]\u001b[0m Trial 20 finished with value: 0.7170777936968958 and parameters: {'num_leaves': 58}. Best is trial 7 with value: 0.7167337441934575.\u001b[0m\nnum_leaves, val_score: 0.716734:  70%|#######   | 14/20 [00:30<00:11,  1.92s/it]","output_type":"stream"},{"name":"stdout","text":"[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.017420 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 3881\n[LightGBM] [Info] Number of data points in the train set: 210000, number of used features: 25\n[LightGBM] [Info] Start training from score 2.109188\n","output_type":"stream"},{"name":"stderr","text":"num_leaves, val_score: 0.716734:  75%|#######5  | 15/20 [00:32<00:09,  1.96s/it]\u001b[32m[I 2021-08-25 23:56:41,585]\u001b[0m Trial 21 finished with value: 0.7172203577213884 and parameters: {'num_leaves': 105}. Best is trial 7 with value: 0.7167337441934575.\u001b[0m\nnum_leaves, val_score: 0.716734:  75%|#######5  | 15/20 [00:32<00:09,  1.96s/it]","output_type":"stream"},{"name":"stdout","text":"[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.018559 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 3881\n[LightGBM] [Info] Number of data points in the train set: 210000, number of used features: 25\n[LightGBM] [Info] Start training from score 2.109188\n","output_type":"stream"},{"name":"stderr","text":"num_leaves, val_score: 0.716734:  80%|########  | 16/20 [00:34<00:07,  1.92s/it]\u001b[32m[I 2021-08-25 23:56:43,402]\u001b[0m Trial 22 finished with value: 0.716924444036563 and parameters: {'num_leaves': 32}. Best is trial 7 with value: 0.7167337441934575.\u001b[0m\nnum_leaves, val_score: 0.716734:  80%|########  | 16/20 [00:34<00:07,  1.92s/it]","output_type":"stream"},{"name":"stdout","text":"[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.017284 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 3881\n[LightGBM] [Info] Number of data points in the train set: 210000, number of used features: 25\n[LightGBM] [Info] Start training from score 2.109188\n","output_type":"stream"},{"name":"stderr","text":"num_leaves, val_score: 0.716734:  85%|########5 | 17/20 [00:36<00:05,  1.91s/it]\u001b[32m[I 2021-08-25 23:56:45,299]\u001b[0m Trial 23 finished with value: 0.71689741396273 and parameters: {'num_leaves': 34}. Best is trial 7 with value: 0.7167337441934575.\u001b[0m\nnum_leaves, val_score: 0.716734:  85%|########5 | 17/20 [00:36<00:05,  1.91s/it]","output_type":"stream"},{"name":"stdout","text":"[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.017316 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 3881\n[LightGBM] [Info] Number of data points in the train set: 210000, number of used features: 25\n[LightGBM] [Info] Start training from score 2.109188\n","output_type":"stream"},{"name":"stderr","text":"num_leaves, val_score: 0.716734:  90%|######### | 18/20 [00:38<00:04,  2.04s/it]\u001b[32m[I 2021-08-25 23:56:47,624]\u001b[0m Trial 24 finished with value: 0.7172320200635746 and parameters: {'num_leaves': 113}. Best is trial 7 with value: 0.7167337441934575.\u001b[0m\nnum_leaves, val_score: 0.716734:  90%|######### | 18/20 [00:38<00:04,  2.04s/it]","output_type":"stream"},{"name":"stdout","text":"[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.017128 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 3881\n[LightGBM] [Info] Number of data points in the train set: 210000, number of used features: 25\n[LightGBM] [Info] Start training from score 2.109188\n","output_type":"stream"},{"name":"stderr","text":"num_leaves, val_score: 0.716615:  95%|#########5| 19/20 [00:40<00:01,  1.96s/it]\u001b[32m[I 2021-08-25 23:56:49,401]\u001b[0m Trial 25 finished with value: 0.7166152655889502 and parameters: {'num_leaves': 5}. Best is trial 25 with value: 0.7166152655889502.\u001b[0m\nnum_leaves, val_score: 0.716615:  95%|#########5| 19/20 [00:40<00:01,  1.96s/it]","output_type":"stream"},{"name":"stdout","text":"[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.017090 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 3881\n[LightGBM] [Info] Number of data points in the train set: 210000, number of used features: 25\n[LightGBM] [Info] Start training from score 2.109188\n","output_type":"stream"},{"name":"stderr","text":"num_leaves, val_score: 0.716615: 100%|##########| 20/20 [00:42<00:00,  1.98s/it]\u001b[32m[I 2021-08-25 23:56:51,433]\u001b[0m Trial 26 finished with value: 0.7170537087094573 and parameters: {'num_leaves': 55}. Best is trial 25 with value: 0.7166152655889502.\u001b[0m\nnum_leaves, val_score: 0.716615: 100%|##########| 20/20 [00:42<00:00,  2.14s/it]\nbagging, val_score: 0.716615:   0%|          | 0/10 [00:00<?, ?it/s]","output_type":"stream"},{"name":"stdout","text":"[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.021566 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 3881\n[LightGBM] [Info] Number of data points in the train set: 210000, number of used features: 25\n[LightGBM] [Info] Start training from score 2.109188\n","output_type":"stream"},{"name":"stderr","text":"bagging, val_score: 0.716615:  10%|#         | 1/10 [00:02<00:23,  2.66s/it]\u001b[32m[I 2021-08-25 23:56:54,103]\u001b[0m Trial 27 finished with value: 0.7166206588600575 and parameters: {'bagging_fraction': 0.9027198647985591, 'bagging_freq': 7}. Best is trial 27 with value: 0.7166206588600575.\u001b[0m\nbagging, val_score: 0.716615:  10%|#         | 1/10 [00:02<00:23,  2.66s/it]","output_type":"stream"},{"name":"stdout","text":"[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.018415 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 3881\n[LightGBM] [Info] Number of data points in the train set: 210000, number of used features: 25\n[LightGBM] [Info] Start training from score 2.109188\n","output_type":"stream"},{"name":"stderr","text":"bagging, val_score: 0.716615:  20%|##        | 2/10 [00:04<00:16,  2.07s/it]\u001b[32m[I 2021-08-25 23:56:55,752]\u001b[0m Trial 28 finished with value: 0.7166439903777764 and parameters: {'bagging_fraction': 0.696992418069659, 'bagging_freq': 7}. Best is trial 27 with value: 0.7166206588600575.\u001b[0m\nbagging, val_score: 0.716615:  20%|##        | 2/10 [00:04<00:16,  2.07s/it]","output_type":"stream"},{"name":"stdout","text":"[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.017074 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 3881\n[LightGBM] [Info] Number of data points in the train set: 210000, number of used features: 25\n[LightGBM] [Info] Start training from score 2.109188\n","output_type":"stream"},{"name":"stderr","text":"bagging, val_score: 0.716607:  30%|###       | 3/10 [00:06<00:14,  2.11s/it]\u001b[32m[I 2021-08-25 23:56:57,924]\u001b[0m Trial 29 finished with value: 0.7166066639618839 and parameters: {'bagging_fraction': 0.8329084084530491, 'bagging_freq': 3}. Best is trial 29 with value: 0.7166066639618839.\u001b[0m\nbagging, val_score: 0.716607:  30%|###       | 3/10 [00:06<00:14,  2.11s/it]","output_type":"stream"},{"name":"stdout","text":"[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.017667 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 3881\n[LightGBM] [Info] Number of data points in the train set: 210000, number of used features: 25\n[LightGBM] [Info] Start training from score 2.109188\n","output_type":"stream"},{"name":"stderr","text":"bagging, val_score: 0.716597:  40%|####      | 4/10 [00:08<00:11,  1.94s/it]\u001b[32m[I 2021-08-25 23:56:59,595]\u001b[0m Trial 30 finished with value: 0.7165966987035497 and parameters: {'bagging_fraction': 0.6882800067864063, 'bagging_freq': 4}. Best is trial 30 with value: 0.7165966987035497.\u001b[0m\nbagging, val_score: 0.716597:  40%|####      | 4/10 [00:08<00:11,  1.94s/it]","output_type":"stream"},{"name":"stdout","text":"[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.016898 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 3881\n[LightGBM] [Info] Number of data points in the train set: 210000, number of used features: 25\n[LightGBM] [Info] Start training from score 2.109188\n","output_type":"stream"},{"name":"stderr","text":"bagging, val_score: 0.716597:  50%|#####     | 5/10 [00:09<00:09,  1.85s/it]\u001b[32m[I 2021-08-25 23:57:01,294]\u001b[0m Trial 31 finished with value: 0.7166417468971074 and parameters: {'bagging_fraction': 0.4410626580833784, 'bagging_freq': 1}. Best is trial 30 with value: 0.7165966987035497.\u001b[0m\nbagging, val_score: 0.716597:  50%|#####     | 5/10 [00:09<00:09,  1.85s/it]","output_type":"stream"},{"name":"stdout","text":"[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.018111 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 3881\n[LightGBM] [Info] Number of data points in the train set: 210000, number of used features: 25\n[LightGBM] [Info] Start training from score 2.109188\n","output_type":"stream"},{"name":"stderr","text":"bagging, val_score: 0.716597:  60%|######    | 6/10 [00:11<00:07,  1.84s/it]\u001b[32m[I 2021-08-25 23:57:03,098]\u001b[0m Trial 32 finished with value: 0.7166371697840503 and parameters: {'bagging_fraction': 0.9018450297072919, 'bagging_freq': 6}. Best is trial 30 with value: 0.7165966987035497.\u001b[0m\nbagging, val_score: 0.716597:  60%|######    | 6/10 [00:11<00:07,  1.84s/it]","output_type":"stream"},{"name":"stdout","text":"[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.017109 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 3881\n[LightGBM] [Info] Number of data points in the train set: 210000, number of used features: 25\n[LightGBM] [Info] Start training from score 2.109188\n","output_type":"stream"},{"name":"stderr","text":"bagging, val_score: 0.716596:  70%|#######   | 7/10 [00:13<00:05,  1.88s/it]\u001b[32m[I 2021-08-25 23:57:05,065]\u001b[0m Trial 33 finished with value: 0.7165960099791389 and parameters: {'bagging_fraction': 0.8377921942020008, 'bagging_freq': 5}. Best is trial 33 with value: 0.7165960099791389.\u001b[0m\nbagging, val_score: 0.716596:  70%|#######   | 7/10 [00:13<00:05,  1.88s/it]","output_type":"stream"},{"name":"stdout","text":"[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.017027 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 3881\n[LightGBM] [Info] Number of data points in the train set: 210000, number of used features: 25\n[LightGBM] [Info] Start training from score 2.109188\n","output_type":"stream"},{"name":"stderr","text":"bagging, val_score: 0.716596:  80%|########  | 8/10 [00:15<00:03,  1.82s/it]\u001b[32m[I 2021-08-25 23:57:06,769]\u001b[0m Trial 34 finished with value: 0.7166207051421951 and parameters: {'bagging_fraction': 0.661048572197932, 'bagging_freq': 4}. Best is trial 33 with value: 0.7165960099791389.\u001b[0m\nbagging, val_score: 0.716596:  80%|########  | 8/10 [00:15<00:03,  1.82s/it]","output_type":"stream"},{"name":"stdout","text":"[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.018528 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 3881\n[LightGBM] [Info] Number of data points in the train set: 210000, number of used features: 25\n[LightGBM] [Info] Start training from score 2.109188\n","output_type":"stream"},{"name":"stderr","text":"bagging, val_score: 0.716596:  90%|######### | 9/10 [00:16<00:01,  1.74s/it]\u001b[32m[I 2021-08-25 23:57:08,319]\u001b[0m Trial 35 finished with value: 0.7166280033538739 and parameters: {'bagging_fraction': 0.4199150663654121, 'bagging_freq': 5}. Best is trial 33 with value: 0.7165960099791389.\u001b[0m\nbagging, val_score: 0.716596:  90%|######### | 9/10 [00:16<00:01,  1.74s/it]","output_type":"stream"},{"name":"stdout","text":"[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.018167 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 3881\n[LightGBM] [Info] Number of data points in the train set: 210000, number of used features: 25\n[LightGBM] [Info] Start training from score 2.109188\n","output_type":"stream"},{"name":"stderr","text":"bagging, val_score: 0.716596: 100%|##########| 10/10 [00:18<00:00,  1.72s/it]\u001b[32m[I 2021-08-25 23:57:10,002]\u001b[0m Trial 36 finished with value: 0.7166181346202569 and parameters: {'bagging_fraction': 0.7282711347281494, 'bagging_freq': 3}. Best is trial 33 with value: 0.7165960099791389.\u001b[0m\nbagging, val_score: 0.716596: 100%|##########| 10/10 [00:18<00:00,  1.86s/it]\nfeature_fraction_stage2, val_score: 0.716596:   0%|          | 0/3 [00:00<?, ?it/s]","output_type":"stream"},{"name":"stdout","text":"[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.017037 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 3881\n[LightGBM] [Info] Number of data points in the train set: 210000, number of used features: 25\n[LightGBM] [Info] Start training from score 2.109188\n","output_type":"stream"},{"name":"stderr","text":"feature_fraction_stage2, val_score: 0.716596:  33%|###3      | 1/3 [00:01<00:03,  2.00s/it]\u001b[32m[I 2021-08-25 23:57:12,003]\u001b[0m Trial 37 finished with value: 0.7166092267547151 and parameters: {'feature_fraction': 0.9520000000000001}. Best is trial 37 with value: 0.7166092267547151.\u001b[0m\nfeature_fraction_stage2, val_score: 0.716596:  33%|###3      | 1/3 [00:01<00:03,  2.00s/it]","output_type":"stream"},{"name":"stdout","text":"[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.017604 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 3881\n[LightGBM] [Info] Number of data points in the train set: 210000, number of used features: 25\n[LightGBM] [Info] Start training from score 2.109188\n","output_type":"stream"},{"name":"stderr","text":"feature_fraction_stage2, val_score: 0.716596:  67%|######6   | 2/3 [00:03<00:01,  1.98s/it]\u001b[32m[I 2021-08-25 23:57:13,974]\u001b[0m Trial 38 finished with value: 0.7165960099791389 and parameters: {'feature_fraction': 0.9840000000000001}. Best is trial 38 with value: 0.7165960099791389.\u001b[0m\nfeature_fraction_stage2, val_score: 0.716596:  67%|######6   | 2/3 [00:03<00:01,  1.98s/it]","output_type":"stream"},{"name":"stdout","text":"[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.017474 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 3881\n[LightGBM] [Info] Number of data points in the train set: 210000, number of used features: 25\n[LightGBM] [Info] Start training from score 2.109188\n","output_type":"stream"},{"name":"stderr","text":"feature_fraction_stage2, val_score: 0.716596: 100%|##########| 3/3 [00:05<00:00,  2.00s/it]\u001b[32m[I 2021-08-25 23:57:15,990]\u001b[0m Trial 39 finished with value: 0.7166636606153924 and parameters: {'feature_fraction': 0.92}. Best is trial 38 with value: 0.7165960099791389.\u001b[0m\nfeature_fraction_stage2, val_score: 0.716596: 100%|##########| 3/3 [00:05<00:00,  2.00s/it]\nregularization_factors, val_score: 0.716596:   0%|          | 0/20 [00:00<?, ?it/s]","output_type":"stream"},{"name":"stdout","text":"[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.019713 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 3881\n[LightGBM] [Info] Number of data points in the train set: 210000, number of used features: 25\n[LightGBM] [Info] Start training from score 2.109188\n","output_type":"stream"},{"name":"stderr","text":"regularization_factors, val_score: 0.716596:   5%|5         | 1/20 [00:02<00:39,  2.06s/it]\u001b[32m[I 2021-08-25 23:57:18,060]\u001b[0m Trial 40 finished with value: 0.7165965159599601 and parameters: {'lambda_l1': 1.952729492811396e-06, 'lambda_l2': 3.629525560793211}. Best is trial 40 with value: 0.7165965159599601.\u001b[0m\nregularization_factors, val_score: 0.716596:   5%|5         | 1/20 [00:02<00:39,  2.06s/it]","output_type":"stream"},{"name":"stdout","text":"[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.017504 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 3881\n[LightGBM] [Info] Number of data points in the train set: 210000, number of used features: 25\n[LightGBM] [Info] Start training from score 2.109188\n","output_type":"stream"},{"name":"stderr","text":"regularization_factors, val_score: 0.716596:  10%|#         | 2/20 [00:04<00:37,  2.10s/it]\u001b[32m[I 2021-08-25 23:57:20,179]\u001b[0m Trial 41 finished with value: 0.7165960099816064 and parameters: {'lambda_l1': 2.0233807236066497e-06, 'lambda_l2': 0.00012176015849785285}. Best is trial 41 with value: 0.7165960099816064.\u001b[0m\nregularization_factors, val_score: 0.716596:  10%|#         | 2/20 [00:04<00:37,  2.10s/it]","output_type":"stream"},{"name":"stdout","text":"[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.018366 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 3881\n[LightGBM] [Info] Number of data points in the train set: 210000, number of used features: 25\n[LightGBM] [Info] Start training from score 2.109188\n","output_type":"stream"},{"name":"stderr","text":"regularization_factors, val_score: 0.716596:  15%|#5        | 3/20 [00:06<00:34,  2.04s/it]\u001b[32m[I 2021-08-25 23:57:22,161]\u001b[0m Trial 42 finished with value: 0.7165960099841219 and parameters: {'lambda_l1': 1.0333167873788636e-06, 'lambda_l2': 0.0004365737494649277}. Best is trial 41 with value: 0.7165960099816064.\u001b[0m\nregularization_factors, val_score: 0.716596:  15%|#5        | 3/20 [00:06<00:34,  2.04s/it]","output_type":"stream"},{"name":"stdout","text":"[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.017276 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 3881\n[LightGBM] [Info] Number of data points in the train set: 210000, number of used features: 25\n[LightGBM] [Info] Start training from score 2.109188\n","output_type":"stream"},{"name":"stderr","text":"regularization_factors, val_score: 0.716596:  20%|##        | 4/20 [00:08<00:32,  2.03s/it]\u001b[32m[I 2021-08-25 23:57:24,160]\u001b[0m Trial 43 finished with value: 0.7166093488246823 and parameters: {'lambda_l1': 3.843848867337202, 'lambda_l2': 8.914847731196228e-06}. Best is trial 41 with value: 0.7165960099816064.\u001b[0m\nregularization_factors, val_score: 0.716596:  20%|##        | 4/20 [00:08<00:32,  2.03s/it]","output_type":"stream"},{"name":"stdout","text":"[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.017013 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 3881\n[LightGBM] [Info] Number of data points in the train set: 210000, number of used features: 25\n[LightGBM] [Info] Start training from score 2.109188\n","output_type":"stream"},{"name":"stderr","text":"regularization_factors, val_score: 0.716596:  25%|##5       | 5/20 [00:11<00:35,  2.36s/it]\u001b[32m[I 2021-08-25 23:57:27,102]\u001b[0m Trial 44 finished with value: 0.7166038111458851 and parameters: {'lambda_l1': 0.19273831706103925, 'lambda_l2': 0.4710103374694771}. Best is trial 41 with value: 0.7165960099816064.\u001b[0m\nregularization_factors, val_score: 0.716596:  25%|##5       | 5/20 [00:11<00:35,  2.36s/it]","output_type":"stream"},{"name":"stdout","text":"[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.046387 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 3881\n[LightGBM] [Info] Number of data points in the train set: 210000, number of used features: 25\n[LightGBM] [Info] Start training from score 2.109188\n","output_type":"stream"},{"name":"stderr","text":"regularization_factors, val_score: 0.716596:  30%|###       | 6/20 [00:13<00:32,  2.29s/it]\u001b[32m[I 2021-08-25 23:57:29,265]\u001b[0m Trial 45 finished with value: 0.7165960099909536 and parameters: {'lambda_l1': 1.641774088231605e-05, 'lambda_l2': 0.00014879973619715537}. Best is trial 41 with value: 0.7165960099816064.\u001b[0m\nregularization_factors, val_score: 0.716596:  30%|###       | 6/20 [00:13<00:32,  2.29s/it]","output_type":"stream"},{"name":"stdout","text":"[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.018140 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 3881\n[LightGBM] [Info] Number of data points in the train set: 210000, number of used features: 25\n[LightGBM] [Info] Start training from score 2.109188\n","output_type":"stream"},{"name":"stderr","text":"regularization_factors, val_score: 0.716596:  35%|###5      | 7/20 [00:15<00:29,  2.26s/it]\u001b[32m[I 2021-08-25 23:57:31,448]\u001b[0m Trial 46 finished with value: 0.71659601408701 and parameters: {'lambda_l1': 2.805373420194817e-07, 'lambda_l2': 0.4151168591349291}. Best is trial 41 with value: 0.7165960099816064.\u001b[0m\nregularization_factors, val_score: 0.716596:  35%|###5      | 7/20 [00:15<00:29,  2.26s/it]","output_type":"stream"},{"name":"stdout","text":"[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.018281 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 3881\n[LightGBM] [Info] Number of data points in the train set: 210000, number of used features: 25\n[LightGBM] [Info] Start training from score 2.109188\n","output_type":"stream"},{"name":"stderr","text":"regularization_factors, val_score: 0.716596:  40%|####      | 8/20 [00:17<00:26,  2.17s/it]\u001b[32m[I 2021-08-25 23:57:33,433]\u001b[0m Trial 47 finished with value: 0.7166158088814314 and parameters: {'lambda_l1': 0.893938914083997, 'lambda_l2': 0.0005250375357852143}. Best is trial 41 with value: 0.7165960099816064.\u001b[0m\nregularization_factors, val_score: 0.716596:  40%|####      | 8/20 [00:17<00:26,  2.17s/it]","output_type":"stream"},{"name":"stdout","text":"[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.017146 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 3881\n[LightGBM] [Info] Number of data points in the train set: 210000, number of used features: 25\n[LightGBM] [Info] Start training from score 2.109188\n","output_type":"stream"},{"name":"stderr","text":"regularization_factors, val_score: 0.716596:  45%|####5     | 9/20 [00:19<00:23,  2.12s/it]\u001b[32m[I 2021-08-25 23:57:35,429]\u001b[0m Trial 48 finished with value: 0.7165960100255971 and parameters: {'lambda_l1': 6.871071248386336e-05, 'lambda_l2': 0.0002515272630625124}. Best is trial 41 with value: 0.7165960099816064.\u001b[0m\nregularization_factors, val_score: 0.716596:  45%|####5     | 9/20 [00:19<00:23,  2.12s/it]","output_type":"stream"},{"name":"stdout","text":"[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.017178 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 3881\n[LightGBM] [Info] Number of data points in the train set: 210000, number of used features: 25\n[LightGBM] [Info] Start training from score 2.109188\n","output_type":"stream"},{"name":"stderr","text":"regularization_factors, val_score: 0.716596:  50%|#####     | 10/20 [00:21<00:20,  2.08s/it]\u001b[32m[I 2021-08-25 23:57:37,432]\u001b[0m Trial 49 finished with value: 0.716614003218224 and parameters: {'lambda_l1': 0.7814323272005137, 'lambda_l2': 0.010750123634636122}. Best is trial 41 with value: 0.7165960099816064.\u001b[0m\nregularization_factors, val_score: 0.716596:  50%|#####     | 10/20 [00:21<00:20,  2.08s/it]","output_type":"stream"},{"name":"stdout","text":"[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.017137 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 3881\n[LightGBM] [Info] Number of data points in the train set: 210000, number of used features: 25\n[LightGBM] [Info] Start training from score 2.109188\n","output_type":"stream"},{"name":"stderr","text":"regularization_factors, val_score: 0.716596:  55%|#####5    | 11/20 [00:23<00:18,  2.05s/it]\u001b[32m[I 2021-08-25 23:57:39,415]\u001b[0m Trial 50 finished with value: 0.7165960099791587 and parameters: {'lambda_l1': 1.6093056207242448e-08, 'lambda_l2': 4.0700258146726697e-08}. Best is trial 50 with value: 0.7165960099791587.\u001b[0m\nregularization_factors, val_score: 0.716596:  55%|#####5    | 11/20 [00:23<00:18,  2.05s/it]","output_type":"stream"},{"name":"stdout","text":"[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.017158 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 3881\n[LightGBM] [Info] Number of data points in the train set: 210000, number of used features: 25\n[LightGBM] [Info] Start training from score 2.109188\n","output_type":"stream"},{"name":"stderr","text":"regularization_factors, val_score: 0.716596:  60%|######    | 12/20 [00:25<00:16,  2.06s/it]\u001b[32m[I 2021-08-25 23:57:41,508]\u001b[0m Trial 51 finished with value: 0.7165960099791605 and parameters: {'lambda_l1': 1.9032747642479137e-08, 'lambda_l2': 1.5733116788793734e-08}. Best is trial 50 with value: 0.7165960099791587.\u001b[0m\nregularization_factors, val_score: 0.716596:  60%|######    | 12/20 [00:25<00:16,  2.06s/it]","output_type":"stream"},{"name":"stdout","text":"[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.017923 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 3881\n[LightGBM] [Info] Number of data points in the train set: 210000, number of used features: 25\n[LightGBM] [Info] Start training from score 2.109188\n","output_type":"stream"},{"name":"stderr","text":"regularization_factors, val_score: 0.716596:  65%|######5   | 13/20 [00:27<00:14,  2.04s/it]\u001b[32m[I 2021-08-25 23:57:43,486]\u001b[0m Trial 52 finished with value: 0.7165960099791535 and parameters: {'lambda_l1': 1.2806399871411022e-08, 'lambda_l2': 1.2561678294579223e-08}. Best is trial 52 with value: 0.7165960099791535.\u001b[0m\nregularization_factors, val_score: 0.716596:  65%|######5   | 13/20 [00:27<00:14,  2.04s/it]","output_type":"stream"},{"name":"stdout","text":"[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.017992 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 3881\n[LightGBM] [Info] Number of data points in the train set: 210000, number of used features: 25\n[LightGBM] [Info] Start training from score 2.109188\n","output_type":"stream"},{"name":"stderr","text":"regularization_factors, val_score: 0.716596:  70%|#######   | 14/20 [00:29<00:12,  2.02s/it]\u001b[32m[I 2021-08-25 23:57:45,471]\u001b[0m Trial 53 finished with value: 0.7165960099791571 and parameters: {'lambda_l1': 1.4652850204730486e-08, 'lambda_l2': 1.1112153027449025e-08}. Best is trial 52 with value: 0.7165960099791535.\u001b[0m\nregularization_factors, val_score: 0.716596:  70%|#######   | 14/20 [00:29<00:12,  2.02s/it]","output_type":"stream"},{"name":"stdout","text":"[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.017172 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 3881\n[LightGBM] [Info] Number of data points in the train set: 210000, number of used features: 25\n[LightGBM] [Info] Start training from score 2.109188\n","output_type":"stream"},{"name":"stderr","text":"regularization_factors, val_score: 0.716596:  75%|#######5  | 15/20 [00:31<00:10,  2.02s/it]\u001b[32m[I 2021-08-25 23:57:47,486]\u001b[0m Trial 54 finished with value: 0.7165960128733204 and parameters: {'lambda_l1': 0.004606355311482662, 'lambda_l2': 2.139362883989869e-07}. Best is trial 52 with value: 0.7165960099791535.\u001b[0m\nregularization_factors, val_score: 0.716596:  75%|#######5  | 15/20 [00:31<00:10,  2.02s/it]","output_type":"stream"},{"name":"stdout","text":"[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.018114 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 3881\n[LightGBM] [Info] Number of data points in the train set: 210000, number of used features: 25\n[LightGBM] [Info] Start training from score 2.109188\n","output_type":"stream"},{"name":"stderr","text":"regularization_factors, val_score: 0.716596:  80%|########  | 16/20 [00:33<00:08,  2.00s/it]\u001b[32m[I 2021-08-25 23:57:49,456]\u001b[0m Trial 55 finished with value: 0.7165960099790472 and parameters: {'lambda_l1': 1.800928888186318e-08, 'lambda_l2': 5.391958353979704e-07}. Best is trial 55 with value: 0.7165960099790472.\u001b[0m\nregularization_factors, val_score: 0.716596:  80%|########  | 16/20 [00:33<00:08,  2.00s/it]","output_type":"stream"},{"name":"stdout","text":"[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.017498 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 3881\n[LightGBM] [Info] Number of data points in the train set: 210000, number of used features: 25\n[LightGBM] [Info] Start training from score 2.109188\n","output_type":"stream"},{"name":"stderr","text":"regularization_factors, val_score: 0.716596:  85%|########5 | 17/20 [00:35<00:05,  2.00s/it]\u001b[32m[I 2021-08-25 23:57:51,441]\u001b[0m Trial 56 finished with value: 0.7165960109751616 and parameters: {'lambda_l1': 0.00158881034774397, 'lambda_l2': 8.734464643092094e-07}. Best is trial 55 with value: 0.7165960099790472.\u001b[0m\nregularization_factors, val_score: 0.716596:  85%|########5 | 17/20 [00:35<00:05,  2.00s/it]","output_type":"stream"},{"name":"stdout","text":"[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.032113 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 3881\n[LightGBM] [Info] Number of data points in the train set: 210000, number of used features: 25\n[LightGBM] [Info] Start training from score 2.109188\n","output_type":"stream"},{"name":"stderr","text":"regularization_factors, val_score: 0.716596:  90%|######### | 18/20 [00:37<00:04,  2.03s/it]\u001b[32m[I 2021-08-25 23:57:53,532]\u001b[0m Trial 57 finished with value: 0.7165960099791586 and parameters: {'lambda_l1': 1.281163733978499e-07, 'lambda_l2': 2.307021304775448e-06}. Best is trial 55 with value: 0.7165960099790472.\u001b[0m\nregularization_factors, val_score: 0.716596:  90%|######### | 18/20 [00:37<00:04,  2.03s/it]","output_type":"stream"},{"name":"stdout","text":"[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.019539 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 3881\n[LightGBM] [Info] Number of data points in the train set: 210000, number of used features: 25\n[LightGBM] [Info] Start training from score 2.109188\n","output_type":"stream"},{"name":"stderr","text":"regularization_factors, val_score: 0.716596:  95%|#########5| 19/20 [00:39<00:02,  2.01s/it]\u001b[32m[I 2021-08-25 23:57:55,503]\u001b[0m Trial 58 finished with value: 0.7165960099791158 and parameters: {'lambda_l1': 1.0083631371100928e-07, 'lambda_l2': 1.0921260422391831e-07}. Best is trial 55 with value: 0.7165960099790472.\u001b[0m\nregularization_factors, val_score: 0.716596:  95%|#########5| 19/20 [00:39<00:02,  2.01s/it]","output_type":"stream"},{"name":"stdout","text":"[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.016803 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 3881\n[LightGBM] [Info] Number of data points in the train set: 210000, number of used features: 25\n[LightGBM] [Info] Start training from score 2.109188\n","output_type":"stream"},{"name":"stderr","text":"regularization_factors, val_score: 0.716596: 100%|##########| 20/20 [00:41<00:00,  2.02s/it]\u001b[32m[I 2021-08-25 23:57:57,535]\u001b[0m Trial 59 finished with value: 0.7165960099898592 and parameters: {'lambda_l1': 1.6976871525927965e-05, 'lambda_l2': 1.5655037854174798e-07}. Best is trial 55 with value: 0.7165960099790472.\u001b[0m\nregularization_factors, val_score: 0.716596: 100%|##########| 20/20 [00:41<00:00,  2.08s/it]\nmin_data_in_leaf, val_score: 0.716596:   0%|          | 0/5 [00:00<?, ?it/s]","output_type":"stream"},{"name":"stdout","text":"[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.271048 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 3881\n[LightGBM] [Info] Number of data points in the train set: 210000, number of used features: 25\n[LightGBM] [Info] Start training from score 2.109188\n","output_type":"stream"},{"name":"stderr","text":"min_data_in_leaf, val_score: 0.716596:  20%|##        | 1/5 [00:02<00:10,  2.62s/it]\u001b[32m[I 2021-08-25 23:58:00,170]\u001b[0m Trial 60 finished with value: 0.7166078387224215 and parameters: {'min_child_samples': 100}. Best is trial 60 with value: 0.7166078387224215.\u001b[0m\nmin_data_in_leaf, val_score: 0.716596:  20%|##        | 1/5 [00:02<00:10,  2.62s/it]","output_type":"stream"},{"name":"stdout","text":"[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.021277 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 3881\n[LightGBM] [Info] Number of data points in the train set: 210000, number of used features: 25\n[LightGBM] [Info] Start training from score 2.109188\n","output_type":"stream"},{"name":"stderr","text":"min_data_in_leaf, val_score: 0.716596:  40%|####      | 2/5 [00:04<00:06,  2.29s/it]\u001b[32m[I 2021-08-25 23:58:02,219]\u001b[0m Trial 61 finished with value: 0.7165963173783977 and parameters: {'min_child_samples': 25}. Best is trial 61 with value: 0.7165963173783977.\u001b[0m\nmin_data_in_leaf, val_score: 0.716596:  40%|####      | 2/5 [00:04<00:06,  2.29s/it]","output_type":"stream"},{"name":"stdout","text":"[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.019752 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 3881\n[LightGBM] [Info] Number of data points in the train set: 210000, number of used features: 25\n[LightGBM] [Info] Start training from score 2.109188\n","output_type":"stream"},{"name":"stderr","text":"min_data_in_leaf, val_score: 0.716596:  60%|######    | 3/5 [00:06<00:04,  2.21s/it]\u001b[32m[I 2021-08-25 23:58:04,339]\u001b[0m Trial 62 finished with value: 0.7165960185843427 and parameters: {'min_child_samples': 50}. Best is trial 62 with value: 0.7165960185843427.\u001b[0m\nmin_data_in_leaf, val_score: 0.716596:  60%|######    | 3/5 [00:06<00:04,  2.21s/it]","output_type":"stream"},{"name":"stdout","text":"[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.021452 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 3881\n[LightGBM] [Info] Number of data points in the train set: 210000, number of used features: 25\n[LightGBM] [Info] Start training from score 2.109188\n","output_type":"stream"},{"name":"stderr","text":"min_data_in_leaf, val_score: 0.716591:  80%|########  | 4/5 [00:08<00:02,  2.13s/it]\u001b[32m[I 2021-08-25 23:58:06,335]\u001b[0m Trial 63 finished with value: 0.716590683255603 and parameters: {'min_child_samples': 10}. Best is trial 63 with value: 0.716590683255603.\u001b[0m\nmin_data_in_leaf, val_score: 0.716591:  80%|########  | 4/5 [00:08<00:02,  2.13s/it]","output_type":"stream"},{"name":"stdout","text":"[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.016974 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 3881\n[LightGBM] [Info] Number of data points in the train set: 210000, number of used features: 25\n[LightGBM] [Info] Start training from score 2.109188\n","output_type":"stream"},{"name":"stderr","text":"min_data_in_leaf, val_score: 0.716591: 100%|##########| 5/5 [00:10<00:00,  2.07s/it]\u001b[32m[I 2021-08-25 23:58:08,308]\u001b[0m Trial 64 finished with value: 0.7165961659700483 and parameters: {'min_child_samples': 5}. Best is trial 63 with value: 0.716590683255603.\u001b[0m\nmin_data_in_leaf, val_score: 0.716591: 100%|##########| 5/5 [00:10<00:00,  2.15s/it]\n","output_type":"stream"},{"execution_count":57,"output_type":"execute_result","data":{"text/plain":"{'objective': 'poisson',\n 'metric': 'rmse',\n 'feature_pre_filter': False,\n 'lambda_l1': 1.800928888186318e-08,\n 'lambda_l2': 5.391958353979704e-07,\n 'num_leaves': 5,\n 'feature_fraction': 1.0,\n 'bagging_fraction': 0.8377921942020008,\n 'bagging_freq': 5,\n 'min_child_samples': 10,\n 'num_iterations': 100,\n 'early_stopping_round': 5}"},"metadata":{}}]},{"cell_type":"code","source":"model_params = {'objective': 'poisson',\n             'metric': 'rmse',\n             'feature_pre_filter': False,\n             'lambda_l1': 1.800928888186318e-08,\n             'lambda_l2': 5.391958353979704e-07,\n             'num_leaves': 5,\n             'feature_fraction': 1.0,\n             'bagging_fraction': 0.8377921942020008,\n             'bagging_freq': 5,\n             'min_child_samples': 10,\n             'num_iterations': 100,\n             'early_stopping_round': 5,\n              \"learning_rate\": 0.005,\n            \"num_iterations\":80000,\n            \"early_stopping_round\": 200}","metadata":{"execution":{"iopub.status.busy":"2021-08-26T00:04:03.064517Z","iopub.execute_input":"2021-08-26T00:04:03.066570Z","iopub.status.idle":"2021-08-26T00:04:03.073337Z","shell.execute_reply.started":"2021-08-26T00:04:03.065019Z","shell.execute_reply":"2021-08-26T00:04:03.072357Z"},"trusted":true},"execution_count":62,"outputs":[]},{"cell_type":"code","source":"final_preds = np.zeros(test.shape[0])\nfinal_train_oof = np.zeros(train.shape[0])\n\nkfd = KFold(n_splits=10, shuffle=True)\n\nfor fold, (train_idx, valid_idx) in enumerate(kfd.split(X=train, y=target)):\n    print(\"-\"*20, f\"FOLD {fold}\", \"-\"*20)\n    X_train, X_valid = train.iloc[train_idx], train.iloc[valid_idx]\n    y_train, y_valid = target.iloc[train_idx], target.iloc[valid_idx]\n    \n    train_dataset = lgb.Dataset(X_train, y_train)\n    valid_dataset = lgb.Dataset(X_valid, y_valid)\n    \n    model = lgb.train(model_params, train_dataset, valid_sets=[valid_dataset], verbose_eval=1000)\n    \n    final_train_oof[valid_idx] = model.predict(X_valid)\n    final_preds += model.predict(test) / 10","metadata":{"execution":{"iopub.status.busy":"2021-08-26T00:04:03.817495Z","iopub.execute_input":"2021-08-26T00:04:03.817896Z","iopub.status.idle":"2021-08-26T00:11:44.615525Z","shell.execute_reply.started":"2021-08-26T00:04:03.817863Z","shell.execute_reply":"2021-08-26T00:11:44.614613Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":63,"outputs":[{"name":"stdout","text":"-------------------- FOLD 0 --------------------\n[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.026455 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 3881\n[LightGBM] [Info] Number of data points in the train set: 270000, number of used features: 25\n[LightGBM] [Info] Start training from score 2.109218\nTraining until validation scores don't improve for 200 rounds\n[1000]\tvalid_0's rmse: 0.715998\nEarly stopping, best iteration is:\n[1550]\tvalid_0's rmse: 0.715766\n-------------------- FOLD 1 --------------------\n[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.022444 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 3881\n[LightGBM] [Info] Number of data points in the train set: 270000, number of used features: 25\n[LightGBM] [Info] Start training from score 2.109316\nTraining until validation scores don't improve for 200 rounds\n[1000]\tvalid_0's rmse: 0.715205\n[2000]\tvalid_0's rmse: 0.714771\nEarly stopping, best iteration is:\n[2470]\tvalid_0's rmse: 0.714757\n-------------------- FOLD 2 --------------------\n[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.022723 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 3881\n[LightGBM] [Info] Number of data points in the train set: 270000, number of used features: 25\n[LightGBM] [Info] Start training from score 2.109230\nTraining until validation scores don't improve for 200 rounds\n[1000]\tvalid_0's rmse: 0.720595\n[2000]\tvalid_0's rmse: 0.720401\nEarly stopping, best iteration is:\n[1800]\tvalid_0's rmse: 0.720399\n-------------------- FOLD 3 --------------------\n[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.024509 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 3881\n[LightGBM] [Info] Number of data points in the train set: 270000, number of used features: 25\n[LightGBM] [Info] Start training from score 2.109326\nTraining until validation scores don't improve for 200 rounds\n[1000]\tvalid_0's rmse: 0.715827\n[2000]\tvalid_0's rmse: 0.715437\nEarly stopping, best iteration is:\n[2055]\tvalid_0's rmse: 0.715435\n-------------------- FOLD 4 --------------------\n[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.023581 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 3881\n[LightGBM] [Info] Number of data points in the train set: 270000, number of used features: 25\n[LightGBM] [Info] Start training from score 2.109301\nTraining until validation scores don't improve for 200 rounds\n[1000]\tvalid_0's rmse: 0.71531\n[2000]\tvalid_0's rmse: 0.715027\nEarly stopping, best iteration is:\n[2195]\tvalid_0's rmse: 0.71502\n-------------------- FOLD 5 --------------------\n[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.042900 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 3881\n[LightGBM] [Info] Number of data points in the train set: 270000, number of used features: 25\n[LightGBM] [Info] Start training from score 2.109241\nTraining until validation scores don't improve for 200 rounds\n[1000]\tvalid_0's rmse: 0.720495\n[2000]\tvalid_0's rmse: 0.720104\nEarly stopping, best iteration is:\n[2555]\tvalid_0's rmse: 0.720078\n-------------------- FOLD 6 --------------------\n[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.022639 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 3881\n[LightGBM] [Info] Number of data points in the train set: 270000, number of used features: 25\n[LightGBM] [Info] Start training from score 2.109326\nTraining until validation scores don't improve for 200 rounds\n[1000]\tvalid_0's rmse: 0.71576\nEarly stopping, best iteration is:\n[1480]\tvalid_0's rmse: 0.7156\n-------------------- FOLD 7 --------------------\n[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.024154 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 3881\n[LightGBM] [Info] Number of data points in the train set: 270000, number of used features: 25\n[LightGBM] [Info] Start training from score 2.109126\nTraining until validation scores don't improve for 200 rounds\n[1000]\tvalid_0's rmse: 0.720576\n[2000]\tvalid_0's rmse: 0.720263\nEarly stopping, best iteration is:\n[2135]\tvalid_0's rmse: 0.72026\n-------------------- FOLD 8 --------------------\n[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.022347 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 3881\n[LightGBM] [Info] Number of data points in the train set: 270000, number of used features: 25\n[LightGBM] [Info] Start training from score 2.109091\nTraining until validation scores don't improve for 200 rounds\n[1000]\tvalid_0's rmse: 0.720168\n[2000]\tvalid_0's rmse: 0.71974\nEarly stopping, best iteration is:\n[2325]\tvalid_0's rmse: 0.71973\n-------------------- FOLD 9 --------------------\n[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.027313 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 3881\n[LightGBM] [Info] Number of data points in the train set: 270000, number of used features: 25\n[LightGBM] [Info] Start training from score 2.109231\nTraining until validation scores don't improve for 200 rounds\n[1000]\tvalid_0's rmse: 0.719942\n[2000]\tvalid_0's rmse: 0.719495\nEarly stopping, best iteration is:\n[2565]\tvalid_0's rmse: 0.719478\n","output_type":"stream"}]},{"cell_type":"code","source":"predictions = pd.DataFrame()\npredictions[\"id\"] = test.index\npredictions[\"target\"] = final_preds\n\npredictions.to_csv('final_submission.csv', index=False, header=predictions.columns)","metadata":{"execution":{"iopub.status.busy":"2021-08-26T00:14:33.120815Z","iopub.execute_input":"2021-08-26T00:14:33.121207Z","iopub.status.idle":"2021-08-26T00:14:33.864100Z","shell.execute_reply.started":"2021-08-26T00:14:33.121176Z","shell.execute_reply":"2021-08-26T00:14:33.863176Z"},"trusted":true},"execution_count":66,"outputs":[]},{"cell_type":"code","source":"predictions = pd.DataFrame()\npredictions[\"id\"] = train.index\npredictions[\"loss\"] = final_train_oof\n\npredictions.to_csv('final_train_oof.csv', index=False, header=predictions.columns)","metadata":{"execution":{"iopub.status.busy":"2021-08-26T00:12:47.165310Z","iopub.execute_input":"2021-08-26T00:12:47.165676Z","iopub.status.idle":"2021-08-26T00:12:48.282719Z","shell.execute_reply.started":"2021-08-26T00:12:47.165646Z","shell.execute_reply":"2021-08-26T00:12:48.281689Z"},"trusted":true},"execution_count":65,"outputs":[]},{"cell_type":"code","source":"Count_cols","metadata":{"execution":{"iopub.status.busy":"2021-08-26T00:18:14.015241Z","iopub.execute_input":"2021-08-26T00:18:14.016044Z","iopub.status.idle":"2021-08-26T00:18:14.022252Z","shell.execute_reply.started":"2021-08-26T00:18:14.015971Z","shell.execute_reply":"2021-08-26T00:18:14.021360Z"},"trusted":true},"execution_count":67,"outputs":[{"execution_count":67,"output_type":"execute_result","data":{"text/plain":"Index(['cont0', 'cont1', 'cont2', 'cont3', 'cont4', 'cont5', 'cont6', 'cont7',\n       'cont8', 'cont9', 'cont10', 'cont11', 'cont12', 'cont13', 'oof'],\n      dtype='object')"},"metadata":{}}]},{"cell_type":"code","source":"N_ESTIMATORS = 20000\nlr = 0.01\nSEED = 299792458\nSPLITS = 10\nVERBOSE = N_ESTIMATORS * 2\nEARLY_STOPPING_ROUNDS = 100","metadata":{"execution":{"iopub.status.busy":"2021-08-26T00:43:16.602903Z","iopub.execute_input":"2021-08-26T00:43:16.603239Z","iopub.status.idle":"2021-08-26T00:43:16.607727Z","shell.execute_reply.started":"2021-08-26T00:43:16.603210Z","shell.execute_reply":"2021-08-26T00:43:16.606646Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"xgb_params = {\n    'random_state': 1, \n    'n_jobs': 4,\n    'booster': 'gbtree',\n    'n_estimators': 10000,\n    'learning_rate': 0.034682894846408095,\n    'reg_lambda': 1.224383455634919,\n    'reg_alpha': 36.043214512614476,\n    'subsample': 0.9219010649982458,\n    'colsample_bytree': 0.11247495917687526,\n    'max_depth': 3,\n    'min_child_weight': 6,\n    \"tree_method\":'gpu_hist',\n    \"gpu_id\":0,\n}","metadata":{"execution":{"iopub.status.busy":"2021-08-26T01:33:27.004968Z","iopub.execute_input":"2021-08-26T01:33:27.005304Z","iopub.status.idle":"2021-08-26T01:33:27.011298Z","shell.execute_reply.started":"2021-08-26T01:33:27.005273Z","shell.execute_reply":"2021-08-26T01:33:27.010258Z"},"trusted":true},"execution_count":78,"outputs":[]},{"cell_type":"code","source":"xgb_oof = np.zeros(train.shape[0])\n#ctb_oof = np.zeros(train.shape[0])\nlr_oof = np.zeros(train.shape[0])\nelst_oof = np.zeros(train.shape[0])\n\n\nxgb_pred = np.zeros(test.shape[0])\n#ctb_pred = np.zeros(test.shape[0])\nlr_pred = np.zeros(test.shape[0])\nelst_pred = np.zeros(test.shape[0])\n\n\nkfd = KFold(n_splits=SPLITS, shuffle=True, random_state=SEED)\n\nfor fold, (train_idx, valid_idx) in enumerate(kfd.split(X=train, y=target)):\n    print(f\"-------------------------- FOLD {fold} --------------------------\")\n    X_train, X_valid = train.iloc[train_idx], train.iloc[valid_idx]\n    y_train, y_valid = target.iloc[train_idx], target.iloc[valid_idx]\n    \n    xgb_model = xgb.XGBRegressor(**xgb_params)\n#     xgb_model = xgb.XGBRegressor(\n#             n_estimators=10000,\n#             learning_rate=lr,\n#             tree_method='gpu_hist',\n#             gpu_id=0,\n#             max_depth=2,\n#             subsample=0.98,\n#             colsample_bytree=0.10,\n#             n_jobs=4,\n#             booster= 'gbtree', \n#             reg_lambda= 66.1,\n#             reg_alpha= 15.9,\n#         )\n    xgb_model.fit(X_train,\n                  y_train,\n                  eval_set=[(X_valid, y_valid)],\n                  eval_metric=\"rmse\",\n                  early_stopping_rounds=EARLY_STOPPING_ROUNDS,\n                  verbose=VERBOSE)\n    \n    valid_preds = xgb_model.predict(X_valid)\n    xgb_oof[valid_idx]  = valid_preds\n    xgb_pred +=  xgb_model.predict(test) / SPLITS\n    \n    print(f\"FOLD: {fold} ---- Valid accuracy XGB: {mean_squared_error(valid_preds, y_valid, squared=False)}\")\n    \n    #------------------------------------------------------------------------------------------------------\n    #------------------------------------------------------------------------------------------------------\n    \n#     ctb_model = ctb.CatBoostRegressor(**ctb_params)\n#     ctb_model.fit(X_train,\n#                   y_train,\n#                   eval_set=[(X_valid, y_valid)],\n#                   use_best_model=True,\n#                   early_stopping_rounds=EARLY_STOPPING_ROUNDS,\n#                   verbose=VERBOSE)\n    \n#     valid_preds = ctb_model.predict(X_valid)\n#     ctb_oof[valid_idx]  = valid_preds\n#     ctb_pred += ctb_model.predict(test) / SPLITS\n    \n#     print(f\"FOLD: {fold} ---- Valid accuracy CTB: {mean_squared_error(valid_preds, y_valid, squared=False)}\")\n    \n     #------------------------------------------------------------------------------------------------------\n    \n    lr_model = LinearRegression()\n    lr_model.fit(X_train,y_train)\n    \n    valid_preds = lr_model.predict(X_valid)\n    lr_oof[valid_idx]  = valid_preds\n    lr_pred += lr_model.predict(test) / SPLITS\n\n    \n    print(f\"FOLD: {fold} ---- Valid accuracy Linear regression: {mean_squared_error(valid_preds, y_valid, squared=False)}\")\n    \n     #------------------------------------------------------------------------------------------------------\n      \n    elst_model = ElasticNet(alpha=0.00001)\n    elst_model.fit(X_train,y_train)\n    \n    valid_preds = elst_model.predict(X_valid)\n    elst_oof[valid_idx]  = valid_preds\n    elst_pred += elst_model.predict(test) / SPLITS\n\n    \n    print(f\"FOLD: {fold} ---- Valid accuracy ElasticNet: {mean_squared_error(valid_preds, y_valid, squared=False)}\")\n    \n     #------------------------------------------------------------------------------------------------------\n    \n    \nprint(f\"oof xgb_rmse = {mean_squared_error(target, xgb_oof, squared=False)}\")\nprint(f\"oof ctb_rmse = {mean_squared_error(target, ctb_oof, squared=False)}\")\nprint(f\"oof lr_rmse = {mean_squared_error(target, lr_oof, squared=False)}\")\nprint(f\"oof elst_rmse = {mean_squared_error(target, elst_oof, squared=False)}\")","metadata":{"execution":{"iopub.status.busy":"2021-08-26T01:33:29.064588Z","iopub.execute_input":"2021-08-26T01:33:29.064952Z","iopub.status.idle":"2021-08-26T01:34:27.254918Z","shell.execute_reply.started":"2021-08-26T01:33:29.064910Z","shell.execute_reply":"2021-08-26T01:34:27.253527Z"},"trusted":true},"execution_count":79,"outputs":[{"name":"stdout","text":"-------------------------- FOLD 0 --------------------------\n[0]\tvalidation_0-rmse:7.51541\n[1178]\tvalidation_0-rmse:0.71757\nFOLD: 0 ---- Valid accuracy XGB: 0.7175654748785141\nFOLD: 0 ---- Valid accuracy Linear regression: 0.7169788640761806\nFOLD: 0 ---- Valid accuracy ElasticNet: 0.7169797298238608\n-------------------------- FOLD 1 --------------------------\n[0]\tvalidation_0-rmse:7.51123\n[1180]\tvalidation_0-rmse:0.71658\nFOLD: 1 ---- Valid accuracy XGB: 0.7165790611883732\nFOLD: 1 ---- Valid accuracy Linear regression: 0.7158004059863663\nFOLD: 1 ---- Valid accuracy ElasticNet: 0.7158001175411398\n-------------------------- FOLD 2 --------------------------\n[0]\tvalidation_0-rmse:7.51035\n[1136]\tvalidation_0-rmse:0.72069\nFOLD: 2 ---- Valid accuracy XGB: 0.7206752557949039\nFOLD: 2 ---- Valid accuracy Linear regression: 0.7201322444572431\nFOLD: 2 ---- Valid accuracy ElasticNet: 0.7201307076179574\n-------------------------- FOLD 3 --------------------------\n[0]\tvalidation_0-rmse:7.50438\n[1270]\tvalidation_0-rmse:0.71853\nFOLD: 3 ---- Valid accuracy XGB: 0.7185284096314464\nFOLD: 3 ---- Valid accuracy Linear regression: 0.7181331946863728\nFOLD: 3 ---- Valid accuracy ElasticNet: 0.7181313837124893\n-------------------------- FOLD 4 --------------------------\n[0]\tvalidation_0-rmse:7.51166\n[1141]\tvalidation_0-rmse:0.72252\nFOLD: 4 ---- Valid accuracy XGB: 0.7225044849968476\nFOLD: 4 ---- Valid accuracy Linear regression: 0.7221739338535322\nFOLD: 4 ---- Valid accuracy ElasticNet: 0.7221715698449352\n-------------------------- FOLD 5 --------------------------\n[0]\tvalidation_0-rmse:7.50439\n[1328]\tvalidation_0-rmse:0.72029\nFOLD: 5 ---- Valid accuracy XGB: 0.7202874305309345\nFOLD: 5 ---- Valid accuracy Linear regression: 0.7195977034437409\nFOLD: 5 ---- Valid accuracy ElasticNet: 0.7195973858818417\n-------------------------- FOLD 6 --------------------------\n[0]\tvalidation_0-rmse:7.50755\n[1419]\tvalidation_0-rmse:0.71902\nFOLD: 6 ---- Valid accuracy XGB: 0.7190072103214968\nFOLD: 6 ---- Valid accuracy Linear regression: 0.7184562361662257\nFOLD: 6 ---- Valid accuracy ElasticNet: 0.7184554186347445\n-------------------------- FOLD 7 --------------------------\n[0]\tvalidation_0-rmse:7.51066\n[1464]\tvalidation_0-rmse:0.71389\nFOLD: 7 ---- Valid accuracy XGB: 0.7138882820393282\nFOLD: 7 ---- Valid accuracy Linear regression: 0.7132255686095003\nFOLD: 7 ---- Valid accuracy ElasticNet: 0.7132246309870013\n-------------------------- FOLD 8 --------------------------\n[0]\tvalidation_0-rmse:7.51379\n[979]\tvalidation_0-rmse:0.71988\nFOLD: 8 ---- Valid accuracy XGB: 0.7198711030308818\nFOLD: 8 ---- Valid accuracy Linear regression: 0.7191863702827399\nFOLD: 8 ---- Valid accuracy ElasticNet: 0.7191841005555074\n-------------------------- FOLD 9 --------------------------\n[0]\tvalidation_0-rmse:7.51713\n[1054]\tvalidation_0-rmse:0.71510\nFOLD: 9 ---- Valid accuracy XGB: 0.7150710877330853\nFOLD: 9 ---- Valid accuracy Linear regression: 0.7143975949014723\nFOLD: 9 ---- Valid accuracy ElasticNet: 0.7143970540445932\noof xgb_rmse = 0.7184022054582823\noof ctb_rmse = 0.7181171666082202\noof lr_rmse = 0.7178129092531457\noof elst_rmse = 0.7178119057085386\n","output_type":"stream"}]},{"cell_type":"code","source":"oof_dataset = pd.DataFrame()\noof_dataset[\"xgb\"] = xgb_oof\n#oof_dataset[\"ctb\"] = ctb_oof\noof_dataset[\"lr\"] = lr_oof\noof_dataset[\"elst\"] = elst_oof\ntmp = pd.read_csv(\"/kaggle/input/optuna-lgb/final_train_oof.csv\", index_col=\"id\")\ntmp[\"id\"] = oof_dataset.index\ntmp = tmp.set_index(\"id\")\noof_dataset[\"lgb\"]  = tmp\n\npred_dataset = pd.DataFrame()\npred_dataset[\"xgb\"] = xgb_pred\n#pred_dataset[\"ctb\"] = ctb_pred\npred_dataset[\"lr\"] = lr_pred\npred_dataset[\"elst\"] = elst_pred\ntmp = pd.read_csv(\"/kaggle/input/optuna-lgb/final_submission.csv\", index_col=\"id\")\ntmp[\"id\"] = pred_dataset.index\ntmp = tmp.set_index(\"id\")\npred_dataset[\"lgb\"] = tmp","metadata":{"execution":{"iopub.status.busy":"2021-08-26T01:34:46.303263Z","iopub.execute_input":"2021-08-26T01:34:46.303587Z","iopub.status.idle":"2021-08-26T01:34:46.546237Z","shell.execute_reply.started":"2021-08-26T01:34:46.303556Z","shell.execute_reply":"2021-08-26T01:34:46.545400Z"},"trusted":true},"execution_count":80,"outputs":[]},{"cell_type":"code","source":"pred_dataset.to_csv(\"pred_dataset.csv\")\noof_dataset.to_csv(\"oof_dataset.csv\")","metadata":{"execution":{"iopub.status.busy":"2021-08-26T01:34:47.301787Z","iopub.execute_input":"2021-08-26T01:34:47.302116Z","iopub.status.idle":"2021-08-26T01:34:51.270618Z","shell.execute_reply.started":"2021-08-26T01:34:47.302087Z","shell.execute_reply":"2021-08-26T01:34:51.269596Z"},"trusted":true},"execution_count":81,"outputs":[]},{"cell_type":"code","source":"sns.displot(oof_dataset, kind=\"kde\")","metadata":{"execution":{"iopub.status.busy":"2021-08-26T01:34:51.272804Z","iopub.execute_input":"2021-08-26T01:34:51.273341Z","iopub.status.idle":"2021-08-26T01:34:58.943085Z","shell.execute_reply.started":"2021-08-26T01:34:51.273295Z","shell.execute_reply":"2021-08-26T01:34:58.942304Z"},"trusted":true},"execution_count":82,"outputs":[{"execution_count":82,"output_type":"execute_result","data":{"text/plain":"<seaborn.axisgrid.FacetGrid at 0x7f060f653250>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<Figure size 414.5x360 with 1 Axes>","image/png":"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\n"},"metadata":{"needs_background":"light"}}]},{"cell_type":"code","source":"meta_model = Lasso(alpha =0.00001)\nstacked_lasso_oof  = np.zeros(train.shape[0])\nstacked_lasso_pred = np.zeros(test.shape[0])\n\n\nkfd = KFold(n_splits=SPLITS, shuffle=True, random_state=SEED)\n\nfor fold, (train_idx, valid_idx) in enumerate(kfd.split(X=oof_dataset, y=target)):\n    print(f\"-------------------------- FOLD {fold} --------------------------\")\n    X_train, X_valid = oof_dataset.iloc[train_idx], oof_dataset.iloc[valid_idx]\n    y_train, y_valid = target.iloc[train_idx], target.iloc[valid_idx]\n                   \n        \n    meta_model = Lasso(alpha =0.00001)\n    meta_model.fit(X_train, y_train)\n    tmp_pred = meta_model.predict(X_valid)\n    stacked_lasso_oof[valid_idx] = tmp_pred\n    stacked_lasso_pred += meta_model.predict(pred_dataset) / SPLITS\n    \n    print(\"RMSE\", mean_squared_error(tmp_pred, y_valid, squared=False))\n    ","metadata":{"execution":{"iopub.status.busy":"2021-08-26T01:35:06.338438Z","iopub.execute_input":"2021-08-26T01:35:06.338780Z","iopub.status.idle":"2021-08-26T01:35:28.335990Z","shell.execute_reply.started":"2021-08-26T01:35:06.338747Z","shell.execute_reply":"2021-08-26T01:35:28.334955Z"},"trusted":true},"execution_count":83,"outputs":[{"name":"stdout","text":"-------------------------- FOLD 0 --------------------------\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:531: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1148.443248629279, tolerance: 15.048089508100238\n  positive)\n","output_type":"stream"},{"name":"stdout","text":"RMSE 0.7160580484378083\n-------------------------- FOLD 1 --------------------------\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:531: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1138.4516788371693, tolerance: 15.05089345627248\n  positive)\n","output_type":"stream"},{"name":"stdout","text":"RMSE 0.7147845735839592\n-------------------------- FOLD 2 --------------------------\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:531: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1155.4215627124504, tolerance: 15.038742550134659\n  positive)\n","output_type":"stream"},{"name":"stdout","text":"RMSE 0.7193304797296167\n-------------------------- FOLD 3 --------------------------\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:531: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1003.8317792606395, tolerance: 15.04036587205772\n  positive)\n","output_type":"stream"},{"name":"stdout","text":"RMSE 0.7175227155001609\n-------------------------- FOLD 4 --------------------------\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:531: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1227.4033894228924, tolerance: 15.033935041943655\n  positive)\n","output_type":"stream"},{"name":"stdout","text":"RMSE 0.721514732108366\n-------------------------- FOLD 5 --------------------------\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:531: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1002.8435381206364, tolerance: 15.045153786679553\n  positive)\n","output_type":"stream"},{"name":"stdout","text":"RMSE 0.7189528671982549\n-------------------------- FOLD 6 --------------------------\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:531: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 936.0518169716815, tolerance: 15.044979331260931\n  positive)\n","output_type":"stream"},{"name":"stdout","text":"RMSE 0.7174823566802743\n-------------------------- FOLD 7 --------------------------\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:531: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 919.4102712635213, tolerance: 15.07158395787907\n  positive)\n","output_type":"stream"},{"name":"stdout","text":"RMSE 0.712320318427783\n-------------------------- FOLD 8 --------------------------\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:531: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1458.9793534293567, tolerance: 15.045543631372654\n  positive)\n","output_type":"stream"},{"name":"stdout","text":"RMSE 0.7187996736785706\n-------------------------- FOLD 9 --------------------------\nRMSE 0.7138816612624252\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:531: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1255.6241180006764, tolerance: 15.063056622585638\n  positive)\n","output_type":"stream"}]},{"cell_type":"code","source":"predictions = pd.DataFrame()\npredictions[\"id\"] = test.index\npredictions[\"target\"] =stacked_lasso_pred\n\npredictions.to_csv('stacked_lasso_pred.csv', index=False, header=predictions.columns)","metadata":{"execution":{"iopub.status.busy":"2021-08-26T01:35:34.359774Z","iopub.execute_input":"2021-08-26T01:35:34.360150Z","iopub.status.idle":"2021-08-26T01:35:34.982920Z","shell.execute_reply.started":"2021-08-26T01:35:34.360118Z","shell.execute_reply":"2021-08-26T01:35:34.981856Z"},"trusted":true},"execution_count":84,"outputs":[]},{"cell_type":"code","source":"pred_dataset[\"xgb\"]","metadata":{"execution":{"iopub.status.busy":"2021-08-26T01:36:05.766289Z","iopub.execute_input":"2021-08-26T01:36:05.766607Z","iopub.status.idle":"2021-08-26T01:36:05.773863Z","shell.execute_reply.started":"2021-08-26T01:36:05.766576Z","shell.execute_reply":"2021-08-26T01:36:05.772829Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":88,"outputs":[{"execution_count":88,"output_type":"execute_result","data":{"text/plain":"0         8.081543\n1         8.409159\n2         8.443038\n3         8.529478\n4         8.158248\n            ...   \n199995    7.975169\n199996    8.494487\n199997    8.490287\n199998    8.130994\n199999    7.956084\nName: xgb, Length: 200000, dtype: float64"},"metadata":{}}]},{"cell_type":"code","source":"predictions = pd.DataFrame()\npredictions[\"id\"] = test.index\npredictions[\"target\"] = pred_dataset[\"xgb\"]\n\npredictions.to_csv('xgboost_submission.csv', index=False, header=predictions.columns)","metadata":{"execution":{"iopub.status.busy":"2021-08-26T01:36:49.770569Z","iopub.execute_input":"2021-08-26T01:36:49.770921Z","iopub.status.idle":"2021-08-26T01:36:50.385315Z","shell.execute_reply.started":"2021-08-26T01:36:49.770890Z","shell.execute_reply":"2021-08-26T01:36:50.384480Z"},"trusted":true},"execution_count":93,"outputs":[]},{"cell_type":"code","source":"predictions = pd.DataFrame()\npredictions[\"id\"] = train.index\npredictions[\"target\"] = stacked_lasso_oof\n\npredictions.to_csv('stacked_lasso_oof.csv', index=False, header=predictions.columns)","metadata":{"execution":{"iopub.status.busy":"2021-08-26T01:36:52.850641Z","iopub.execute_input":"2021-08-26T01:36:52.851003Z","iopub.status.idle":"2021-08-26T01:36:53.939261Z","shell.execute_reply.started":"2021-08-26T01:36:52.850972Z","shell.execute_reply":"2021-08-26T01:36:53.938342Z"},"trusted":true},"execution_count":94,"outputs":[]}]}